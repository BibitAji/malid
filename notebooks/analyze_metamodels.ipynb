{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6108004f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e1210f7-2ea0-4398-8ec5-d0b4ca794c7c",
   "metadata": {},
   "source": [
    "# Analyze metamodel performance on test set, with abstention\n",
    "\n",
    "> Train patient-level rollup model using existing base models trained on train-smaller set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d62b76-ea41-4b6e-ba62-831b438568c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b04f883-d9bb-4fce-83a4-79b3b0dc35ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, Generator, List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import itertools\n",
    "import genetools\n",
    "from statannotations.Annotator import Annotator\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d1d195-26ae-4076-849d-791fa661b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from malid import config, logger, helpers\n",
    "from wrap_glmnet import GlmnetLogitNetWrapper\n",
    "from malid.train import train_metamodel\n",
    "import crosseval\n",
    "from malid.datamodels import (\n",
    "    DataSource,\n",
    "    TargetObsColumnEnum,\n",
    "    GeneLocus,\n",
    "    map_cross_validation_split_strategy_to_default_target_obs_column,\n",
    ")\n",
    "from malid.trained_model_wrappers import BlendingMetamodel\n",
    "\n",
    "# Note: don't import this before seaborn, otherwise the style will be overriden and figures will have a different size:\n",
    "from malid.trained_model_wrappers.blending_metamodel import MetamodelConfig\n",
    "from genetools.plots import (\n",
    "    plot_two_key_color_and_size_dotplot,\n",
    "    plot_triangular_heatmap,\n",
    ")\n",
    "import multiclass_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b333b1ee-776a-4ca0-9a22-c241c3f52f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2f9868-a919-4561-bc11-63cbd87bfd5f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704257e7-1779-4ce2-81e4-4a3695d9b4cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _plot_feature_importances(\n",
    "    plot_df: pd.DataFrame, model_name: str, xlabel: str, xmin_at_zero: bool\n",
    "):\n",
    "    \"\"\"plot feature importances for binary/multiclass random forest or binary linear model,\n",
    "    where we have one model across all classes, rather than OvR multiclass model\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(4, plot_df.shape[1] / 2.5))\n",
    "\n",
    "    try:\n",
    "        # Convert any metamodel feature names to friendly names,\n",
    "        # if they have not already been renamed to friendly names when grouping/summing subsets.\n",
    "        plot_df = plot_df.rename(\n",
    "            columns=lambda feature_name: BlendingMetamodel.convert_feature_name_to_friendly_name(\n",
    "                feature_name\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if plot_df.shape[0] == 1:\n",
    "            # Special case: single entry. Show scatter plot instead of box plot.\n",
    "            ax.scatter(plot_df.iloc[0].values, plot_df.iloc[0].index)\n",
    "            # Make spacing and y-axis order similar to default boxplot\n",
    "            buffer = 0.5\n",
    "            ax.set_ylim(-buffer, plot_df.shape[1] - 1 + buffer)\n",
    "            ax.invert_yaxis()\n",
    "        else:\n",
    "            # Default: boxplot\n",
    "            sns.boxplot(data=plot_df, orient=\"h\", ax=ax)\n",
    "\n",
    "        plt.title(\n",
    "            f\"{model_name} ({plot_df.shape[0]} fold{'s'[:plot_df.shape[0] != 1]})\"\n",
    "        )\n",
    "        plt.xlabel(xlabel)\n",
    "        if xmin_at_zero:\n",
    "            plt.xlim(\n",
    "                0,\n",
    "            )\n",
    "        return fig\n",
    "    except Exception as err:\n",
    "        # close figure just in case, some Jupyter does not try to display a broken figure\n",
    "        plt.close(fig)\n",
    "        # reraise\n",
    "        raise err\n",
    "\n",
    "\n",
    "def _sum_subsets_of_feature_importances(\n",
    "    df: pd.DataFrame,\n",
    "    subset_names: Optional[Dict[str, str]],\n",
    "    drop_empty_subsets: bool = True,\n",
    "):\n",
    "    \"\"\"Sum up feature importances by subsets.\n",
    "    Subset_names is a dict mapping friendly_subset_name to regex to match columns (we match with \"contains\" operation).\n",
    "    Drop_empty_subsets is whether to drop empty subsets (i.e. where no columns match the regex).\n",
    "    Pass through as-is without summing if subset_names is not provided\n",
    "    \"\"\"\n",
    "    if subset_names is not None:\n",
    "        # get relevant columns for each subset\n",
    "        sum_parts = {\n",
    "            name: df.loc[:, df.columns.str.contains(regex)]\n",
    "            for name, regex in subset_names.items()\n",
    "        }\n",
    "\n",
    "        if drop_empty_subsets:\n",
    "            # drop subsets where no columns have matched\n",
    "            sum_parts = {\n",
    "                name: df_part\n",
    "                for name, df_part in sum_parts.items()\n",
    "                if not df_part.empty\n",
    "            }\n",
    "\n",
    "        if len(sum_parts) == 0:\n",
    "            raise ValueError(\n",
    "                f\"Subset names {subset_names} not found in df columns {df.columns}\"\n",
    "            )\n",
    "\n",
    "        # do the sums\n",
    "        return pd.DataFrame.from_dict(\n",
    "            {name: df_part.sum(axis=1) for name, df_part in sum_parts.items()},\n",
    "            orient=\"columns\",\n",
    "        )\n",
    "\n",
    "    # pass through as-is without summing if subset_names is not provided\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_feature_importance_subsets_to_plot(\n",
    "    gene_locus: GeneLocus,\n",
    ") -> Dict[str, Union[Dict[str, str], None]]:\n",
    "    model_component_names = [\n",
    "        (\"repertoire_stats\", \"Repertoire composition\"),\n",
    "        (\"convergent_cluster_model\", \"CDR3 clustering\"),\n",
    "        (\"sequence_model\", \"Language model\"),\n",
    "    ]\n",
    "    demographics_include = {\"Demographics\": \"^demographics\"}\n",
    "    interactions_include = {\n",
    "        \"Sequence x Demographic feature interactions\": \"^interaction\"\n",
    "    }\n",
    "    return {\n",
    "        \"all\": None,\n",
    "        \"by_locus\": {\n",
    "            f\"{gene_locus_part.name}\": f\"^{gene_locus_part.name}:*\"\n",
    "            for gene_locus_part in gene_locus\n",
    "        }\n",
    "        | demographics_include\n",
    "        | interactions_include,\n",
    "        \"by_model_component\": {\n",
    "            # Don't match if starts with interaction\n",
    "            # i.e. match \"BCR:sequence_model:Covid19\" but not \"interaction|BCR:sequence_model:Covid19|demographics:age\".\n",
    "            f\"{model_component_friendly_name}\": f\"^(?:(?!interaction).)*{model_component_name}\"\n",
    "            for model_component_name, model_component_friendly_name in model_component_names\n",
    "        }\n",
    "        | demographics_include\n",
    "        | interactions_include,\n",
    "        \"by_locus_and_model_component\": {\n",
    "            f\"{model_component_friendly_name} ({gene_locus_part.name})\": f\"^{gene_locus_part.name}:{model_component_name}:*\"\n",
    "            for model_component_name, model_component_friendly_name in model_component_names\n",
    "            for gene_locus_part in gene_locus\n",
    "        }\n",
    "        | demographics_include\n",
    "        | interactions_include,\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_multiclass_feature_importances(\n",
    "    model_name: str,\n",
    "    raw_coefs_mean: pd.DataFrame,\n",
    "    raw_coefs_std: Optional[pd.DataFrame],\n",
    "    gene_locus: GeneLocus,\n",
    "    target_obs_column: TargetObsColumnEnum,\n",
    "    metamodel_flavor: str,\n",
    "    n_folds: int,\n",
    ") -> Generator[Tuple[str, plt.Figure], None, None]:\n",
    "    ## We will plot raw coefs, and also use absvals so we can combine features\n",
    "\n",
    "    def _sort_plot_features(features_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Arrange feature columns in desired order:\n",
    "        # 1. BCR : model1 : Covid19\n",
    "        # 2. BCR : model2 : Covid19\n",
    "        # 3. BCR : model3 : Covid19\n",
    "        # 4. TCR : model1 : Covid19\n",
    "        # 5. TCR : model2 : Covid19\n",
    "        # 6. TCR : model3 : Covid19\n",
    "        # 7. BCR : model1 : HIV\n",
    "        # and so on\n",
    "        column_order = features_df.columns.to_series().str.split(\":\", expand=True)\n",
    "        if column_order.shape[1] >= 3:\n",
    "            # this is true for the above examples\n",
    "            column_order = column_order.sort_values([2, 0])\n",
    "        else:\n",
    "            # the demographics-only metamodel flavor has feature names with only one single colon\n",
    "            column_order = column_order.sort_values([0])\n",
    "        return features_df[column_order.index]\n",
    "\n",
    "    raw_coefs_mean = _sort_plot_features(raw_coefs_mean)\n",
    "    if raw_coefs_std is not None:\n",
    "        raw_coefs_std = _sort_plot_features(raw_coefs_std)\n",
    "\n",
    "    diverging_color_cmap = \"RdBu_r\"\n",
    "    # Cut cmap by 15% from both sides, so that we don't have dark blue and dark red at the extremes, which are hard to distinguish\n",
    "    # https://stackoverflow.com/a/18926541/130164\n",
    "    diverging_color_cmap = matplotlib.colormaps.get_cmap(diverging_color_cmap)\n",
    "    diverging_color_cmap = diverging_color_cmap.from_list(\n",
    "        name=f\"{diverging_color_cmap.name}_truncated\",\n",
    "        colors=diverging_color_cmap(np.linspace(0.15, 0.85, 256)),\n",
    "        N=256,\n",
    "    )\n",
    "\n",
    "    def _plot(\n",
    "        features_df: pd.DataFrame,\n",
    "        label: str,\n",
    "        cmap_diverging: bool,\n",
    "        require_sum_to_1: bool,\n",
    "        make_percentage: bool = False,\n",
    "    ):\n",
    "        # autosize\n",
    "        figsize = (features_df.shape[0] * 1.0, features_df.shape[1] / 2.5)\n",
    "\n",
    "        if require_sum_to_1 and not np.allclose(features_df.sum(axis=1), 1):\n",
    "            raise ValueError(\"Sum of feature importances is not 1\")\n",
    "        if make_percentage:\n",
    "            # Turn fractions into percentages\n",
    "            if not require_sum_to_1:\n",
    "                raise ValueError(\"make_percentage requires require_sum_to_1\")\n",
    "            features_df = features_df * 100\n",
    "\n",
    "        # Convert any metamodel feature names to friendly names,\n",
    "        # if they have not already been renamed to friendly names when grouping/summing subsets.\n",
    "        features_df = features_df.rename(\n",
    "            columns=lambda feature_name: BlendingMetamodel.convert_feature_name_to_friendly_name(\n",
    "                feature_name\n",
    "            )\n",
    "        )\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "        try:\n",
    "            # Create dedicated colorbar axis\n",
    "            colorbar_ax = inset_axes(\n",
    "                ax,\n",
    "                width=\"80%\",  # relative unit\n",
    "                height=0.25,  # in inches\n",
    "                loc=\"lower center\",\n",
    "                borderpad=-5,  # create space\n",
    "            )\n",
    "            sns.heatmap(\n",
    "                # plot transpose, so features are on y-axis\n",
    "                features_df.T,\n",
    "                center=0 if cmap_diverging else None,\n",
    "                linewidths=0.5,\n",
    "                cmap=diverging_color_cmap\n",
    "                if cmap_diverging\n",
    "                else \"Blues\",  # sns.color_palette(\"vlag\", as_cmap=True) is another good diverging\n",
    "                ax=ax,\n",
    "                # Put colorbar on bottom\n",
    "                cbar_kws={\"label\": label, \"orientation\": \"horizontal\"},\n",
    "                cbar_ax=colorbar_ax,\n",
    "                # plot all tick labels\n",
    "                xticklabels=True,\n",
    "                yticklabels=True,\n",
    "            )\n",
    "\n",
    "            # Adjust tick labels\n",
    "            ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "            ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "            genetools.plots.wrap_tick_labels(\n",
    "                ax=ax, wrap_x_axis=True, wrap_y_axis=False, wrap_amount=12\n",
    "            )\n",
    "\n",
    "            # set global \"current axes\" back to main axes,\n",
    "            # so that any calls like plt.title target main ax rather than colorbar_ax\n",
    "            plt.sca(ax)\n",
    "            return fig, ax\n",
    "        except Exception as err:\n",
    "            # close figure just in case, some Jupyter does not try to display a broken figure\n",
    "            plt.close(fig)\n",
    "            # reraise\n",
    "            raise err\n",
    "\n",
    "    # Plot mean\n",
    "    try:\n",
    "        fig, ax = _plot(\n",
    "            _sort_plot_features(raw_coefs_mean),\n",
    "            label=\"Coefficient mean\",\n",
    "            cmap_diverging=True,\n",
    "            require_sum_to_1=False,\n",
    "        )\n",
    "        ax.set_title(\n",
    "            f\"Feature coefficients, each class versus the rest (mean over {n_folds} folds)\"\n",
    "        )\n",
    "        yield (f\"raw_coefs.mean\", fig)\n",
    "    except Exception as err:\n",
    "        logger.warning(\n",
    "            f\"Failed to plot {model_name}, {gene_locus}, {target_obs_column}, metamodel flavor {metamodel_flavor} multiclass raw_coefs.mean with error: {err}\"\n",
    "        )\n",
    "\n",
    "    if raw_coefs_std is not None:\n",
    "        # Plot std\n",
    "        try:\n",
    "            fig, ax = _plot(\n",
    "                _sort_plot_features(raw_coefs_std),\n",
    "                label=\"Coefficient stdev\",\n",
    "                cmap_diverging=False,\n",
    "                require_sum_to_1=False,\n",
    "            )\n",
    "            ax.set_title(\n",
    "                f\"Feature coefficients, each class versus the rest (stdev over {n_folds} folds)\"\n",
    "            )\n",
    "            yield (f\"raw_coefs.stdev\", fig)\n",
    "        except Exception as err:\n",
    "            logger.warning(\n",
    "                f\"Failed to plot {model_name}, {gene_locus}, {target_obs_column}, metamodel flavor {metamodel_flavor} multiclass raw_coefs.stdev with error: {err}\"\n",
    "            )\n",
    "\n",
    "        # Plot mean and standard deviation together\n",
    "        combined = pd.merge(\n",
    "            raw_coefs_mean.rename_axis(index=\"class\")\n",
    "            .reset_index()\n",
    "            .melt(\n",
    "                id_vars=[\"class\"],\n",
    "                value_vars=raw_coefs_mean.columns,\n",
    "                var_name=\"feature\",\n",
    "                value_name=\"mean\",\n",
    "            ),\n",
    "            raw_coefs_std.rename_axis(index=\"class\")\n",
    "            .reset_index()\n",
    "            .melt(\n",
    "                id_vars=[\"class\"],\n",
    "                value_vars=raw_coefs_std.columns,\n",
    "                var_name=\"feature\",\n",
    "                value_name=\"stdev\",\n",
    "            ),\n",
    "            on=[\"class\", \"feature\"],\n",
    "            how=\"inner\",\n",
    "            validate=\"1:1\",\n",
    "        )\n",
    "        # Convert raw metamodel feature names to friendly names\n",
    "        combined[\"feature\"] = combined[\"feature\"].apply(\n",
    "            BlendingMetamodel.convert_feature_name_to_friendly_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            fig, ax = plot_two_key_color_and_size_dotplot(\n",
    "                data=combined,\n",
    "                x_axis_key=\"class\",\n",
    "                y_axis_key=\"feature\",\n",
    "                color_key=\"mean\",\n",
    "                size_key=\"stdev\",\n",
    "                color_cmap=diverging_color_cmap,\n",
    "                color_vcenter=0,\n",
    "                inverse_size=True,\n",
    "                color_legend_text=\"mean\",\n",
    "                size_legend_text=\"inverse std. dev.\",\n",
    "                min_marker_size=20,\n",
    "            )\n",
    "            # TODO: make hierarchical y-axis labels (https://stackoverflow.com/questions/19184484/how-to-add-group-labels-for-bar-charts, https://stackoverflow.com/questions/37934242/hierarchical-axis-labeling-in-matplotlib-python)\n",
    "            ax.set_title(\n",
    "                f\"Feature coefficients, each class versus the rest (over {n_folds} folds)\"\n",
    "            )\n",
    "            yield (f\"raw_coefs\", fig)\n",
    "        except Exception as err:\n",
    "            logger.warning(\n",
    "                f\"Failed to plot {model_name}, {gene_locus}, {target_obs_column}, metamodel flavor {metamodel_flavor} multiclass raw_coefs (mean+stdev together) with error: {err}\"\n",
    "            )\n",
    "\n",
    "    ## Report aggregate feature importance of several features in a linear model\n",
    "    # e.g. I'd like to say something about how much all the language model features contribute to the metamodel, vs all the CDR3 clustering features.\n",
    "    # I believe you can [sum feature importances](https://stats.stackexchange.com/questions/311488/summing-feature-importance-in-scikit-learn-for-a-set-of-features) for a set of features in a random forest.\n",
    "    # for a linear model, I suppose I could take the absolute value of the coefs and sum them for something like \"overall effect strength from this set of features\".\n",
    "\n",
    "    # Convert to absolute value, and divide by the sum of absolute values of all coefficients for \"percent contribution\"\n",
    "    normalized_coefs = genetools.stats.normalize_rows(np.abs(raw_coefs_mean))\n",
    "    for fig_name, subset_names in get_feature_importance_subsets_to_plot(\n",
    "        gene_locus\n",
    "    ).items():\n",
    "        # sum up by origin of feature importances and replot.\n",
    "        try:\n",
    "            logger.debug(f\"{model_name} absval_coefs {fig_name} across folds\")\n",
    "            fig, ax = _plot(\n",
    "                _sum_subsets_of_feature_importances(\n",
    "                    df=normalized_coefs, subset_names=subset_names\n",
    "                ),\n",
    "                label=\"Percent contribution\",  # \"Coefficient absval, percent contribution\",\n",
    "                cmap_diverging=False,\n",
    "                require_sum_to_1=True,\n",
    "                make_percentage=True,\n",
    "            )\n",
    "            plt.title(\n",
    "                f\"{model_name} feature percent contributions\\neach class versus the rest\\n(averaged over {n_folds} folds)\"\n",
    "            )\n",
    "            yield (f\"absval_coefs.{fig_name}\", fig)\n",
    "        except Exception as err:\n",
    "            # Skip broken figures\n",
    "            # One possible cause is that the feature names for this metamodel flavor don't correspond to what get_feature_importance_subsets_to_plot() is producing.\n",
    "            logger.warning(\n",
    "                f\"Failed to plot {model_name}, {gene_locus}, {target_obs_column}, metamodel flavor {metamodel_flavor} feature percent contributions for figure name absval_coefs.{fig_name}, subset names {subset_names}: {err}\"\n",
    "            )\n",
    "\n",
    "\n",
    "def analyze_feature_importances(\n",
    "    model_name: str,\n",
    "    model_global_performance: crosseval.ModelGlobalPerformance,\n",
    "    gene_locus: GeneLocus,\n",
    "    target_obs_column: TargetObsColumnEnum,\n",
    "    metamodel_flavor: str,\n",
    "    highres_results_output_prefix: Path,\n",
    "    global_fold_classifier: Optional[BlendingMetamodel],\n",
    "):\n",
    "    \"\"\"Get and analyze feature importances.\"\"\"\n",
    "    # First, check if model is binary in each fold\n",
    "    is_binary = all(\n",
    "        len(per_fold_output.class_names) == 2\n",
    "        for per_fold_output in model_global_performance.per_fold_outputs.values()\n",
    "    )\n",
    "    if (\n",
    "        global_fold_classifier is not None\n",
    "        and (len(global_fold_classifier.classes_) == 2) != is_binary\n",
    "    ):\n",
    "        # Sanity check\n",
    "        logger.warning(\n",
    "            f\"Ignoring global fold classifier for {model_name} because cross validation is_binary={is_binary} does not match global fold classes count = {len(global_fold_classifier.classes_)}\"\n",
    "        )\n",
    "        global_fold_classifier = None\n",
    "\n",
    "    # Depending on the model type (tree vs linear model; binary vs multiclass), we will retrieve and plot feature importances differently.\n",
    "    # (Tree models are always a single model across all classes, regardless of whether classification target is binary or multiclass,\n",
    "    # whereas multiclass linear models may be trained separately for each class.)\n",
    "    is_tree = model_name in [\"rf_multiclass\", \"xgboost\"]\n",
    "    is_linear_model = any(\n",
    "        model_name.startswith(s)\n",
    "        for s in [\n",
    "            \"linearsvm_ovr\",\n",
    "            \"lasso_cv\",\n",
    "            \"ridge_cv\",\n",
    "            \"elasticnet_cv\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if is_tree or (is_linear_model and is_binary):\n",
    "        # Get feature importances for each fold\n",
    "        feature_importances_cross_validation_df: Union[\n",
    "            pd.DataFrame, None\n",
    "        ] = model_global_performance.feature_importances\n",
    "        if feature_importances_cross_validation_df is None:\n",
    "            logger.warning(f\"No feature importances available for {model_name}\")\n",
    "            # skip this model\n",
    "            return\n",
    "        feature_importances_to_plot = [(feature_importances_cross_validation_df, \"\")]\n",
    "\n",
    "        if global_fold_classifier is not None:\n",
    "            global_fold_feature_importances = crosseval._extract_feature_importances(\n",
    "                global_fold_classifier._inner\n",
    "            )\n",
    "            global_fold_feature_names = crosseval._get_feature_names(\n",
    "                global_fold_classifier._inner\n",
    "            )\n",
    "            if global_fold_feature_importances is None:\n",
    "                logger.warning(\n",
    "                    f\"No feature importances available for {model_name} (global fold)\"\n",
    "                )\n",
    "            else:\n",
    "                feature_importances_to_plot.append(\n",
    "                    (\n",
    "                        pd.Series(\n",
    "                            global_fold_feature_importances,\n",
    "                            index=global_fold_feature_names,\n",
    "                            name=-1,\n",
    "                        )\n",
    "                        .to_frame()\n",
    "                        .T,\n",
    "                        \"_global_fold\",\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # Plot feature importances.\n",
    "        for (\n",
    "            feature_importances,\n",
    "            overall_name,\n",
    "        ) in feature_importances_to_plot:\n",
    "            if is_tree:\n",
    "                for (\n",
    "                    fig_name,\n",
    "                    subset_names,\n",
    "                ) in get_feature_importance_subsets_to_plot(gene_locus).items():\n",
    "                    # sum up by origin of feature importances and replot.\n",
    "                    try:\n",
    "                        fig = _plot_feature_importances(\n",
    "                            plot_df=_sum_subsets_of_feature_importances(\n",
    "                                df=feature_importances,\n",
    "                                subset_names=subset_names,\n",
    "                            ),\n",
    "                            model_name=model_name,\n",
    "                            xlabel=\"Feature importance\",\n",
    "                            # Values are all positive for tree models\n",
    "                            xmin_at_zero=True,\n",
    "                        )\n",
    "                        genetools.plots.savefig(\n",
    "                            fig,\n",
    "                            f\"{highres_results_output_prefix}.feature_importances{overall_name}.{model_name}.{fig_name}.png\",\n",
    "                            dpi=300,\n",
    "                        )\n",
    "                        plt.close(fig)\n",
    "                    except Exception as err:\n",
    "                        # Skip broken figures\n",
    "                        # One possible cause is that the feature names for this metamodel flavor don't correspond to what get_feature_importance_subsets_to_plot() is producing.\n",
    "                        logger.warning(\n",
    "                            f\"Failed to plot {model_name} feature importances{overall_name} for {gene_locus}, {target_obs_column}, metamodel flavor {metamodel_flavor}, with figure name {fig_name} and subset names {subset_names}: {err}\"\n",
    "                        )\n",
    "\n",
    "            elif is_linear_model:\n",
    "                for (\n",
    "                    feature_importances,\n",
    "                    overall_name,\n",
    "                ) in feature_importances_to_plot:\n",
    "                    # TODO: Add normalization of coefficients and summing of subsets (nontrivial for linear model)\n",
    "                    # For now only plot all the features - don't group by subset.\n",
    "                    fig = _plot_feature_importances(\n",
    "                        plot_df=feature_importances,\n",
    "                        model_name=model_name,\n",
    "                        xlabel=\"Feature coefficient\",\n",
    "                        # coefficients are not necessarily positive\n",
    "                        xmin_at_zero=False,\n",
    "                    )\n",
    "                    genetools.plots.savefig(\n",
    "                        fig,\n",
    "                        f\"{highres_results_output_prefix}.feature_importances{overall_name}.{model_name}.all.png\",\n",
    "                        dpi=300,\n",
    "                    )\n",
    "                    plt.close(fig)\n",
    "\n",
    "    elif is_linear_model and not is_binary:\n",
    "        # Many OvR models for each class vs the rest\n",
    "        raw_coefs: Optional[\n",
    "            Dict[int, pd.DataFrame]\n",
    "        ] = model_global_performance.multiclass_feature_importances\n",
    "        if raw_coefs is None:\n",
    "            logger.warning(\n",
    "                f\"No feature importances available for multiclass {model_name}\"\n",
    "            )\n",
    "            # skip this model\n",
    "            return\n",
    "\n",
    "        ## Combine multiclass feature importances across folds:\n",
    "        # The coefs are comparable across folds because the inputs to the model were standardized.\n",
    "\n",
    "        # Create 3D array from these 2D arrays - making sure that the index and column order is the same across folds.\n",
    "        first_df = next(iter(raw_coefs.values()))\n",
    "        try:\n",
    "            raw_coefs_data: np.ndarray = np.array(\n",
    "                [df.loc[first_df.index][first_df.columns] for df in raw_coefs.values()]\n",
    "            )\n",
    "        except Exception as err:\n",
    "            logger.warning(\n",
    "                f\"Could not combine feature coefficients across folds for multiclass linear model {model_name} ({gene_locus}, {target_obs_column}, metamodel flavor {metamodel_flavor}), possibly because of missing classes. Skipping feature importance plots with this error: {err}\"\n",
    "            )\n",
    "            # skip this model\n",
    "            return\n",
    "\n",
    "        # Extract mean and standard deviation, and repack in dataframe\n",
    "        raw_coefs_mean: pd.DataFrame = pd.DataFrame(\n",
    "            np.mean(raw_coefs_data, axis=0),\n",
    "            index=first_df.index,\n",
    "            columns=first_df.columns,\n",
    "        )\n",
    "        raw_coefs_std: pd.DataFrame = pd.DataFrame(\n",
    "            np.std(raw_coefs_data, axis=0),\n",
    "            index=first_df.index,\n",
    "            columns=first_df.columns,\n",
    "        )\n",
    "\n",
    "        raw_coefs_mean.to_csv(\n",
    "            f\"{highres_results_output_prefix}.feature_importances.{model_name}.raw_coefs_mean.tsv\",\n",
    "            sep=\"\\t\",\n",
    "        )\n",
    "        raw_coefs_std.to_csv(\n",
    "            f\"{highres_results_output_prefix}.feature_importances.{model_name}.raw_coefs_std.tsv\",\n",
    "            sep=\"\\t\",\n",
    "        )\n",
    "\n",
    "        for fig_name, fig in plot_multiclass_feature_importances(\n",
    "            model_name=model_name,\n",
    "            raw_coefs_mean=raw_coefs_mean,\n",
    "            raw_coefs_std=raw_coefs_std,\n",
    "            gene_locus=gene_locus,\n",
    "            target_obs_column=target_obs_column,\n",
    "            metamodel_flavor=metamodel_flavor,\n",
    "            n_folds=len(raw_coefs),\n",
    "        ):\n",
    "            fname = f\"{highres_results_output_prefix}.feature_importances.{model_name}.{fig_name}.png\"\n",
    "            logger.debug(f\"{fig_name} -> {fname}\")\n",
    "            genetools.plots.savefig(\n",
    "                fig,\n",
    "                fname,\n",
    "                dpi=300,\n",
    "            )\n",
    "            plt.close(fig)\n",
    "\n",
    "        if global_fold_classifier is not None:\n",
    "            # Also plot global fold coefficients on their own. (We will pass them as raw_coefs_mean (without running a mean), with raw_coefs_std set to None)\n",
    "            global_fold_feature_importances = (\n",
    "                crosseval._extract_multiclass_feature_importances(\n",
    "                    global_fold_classifier._inner\n",
    "                )\n",
    "            )\n",
    "            global_fold_feature_names = crosseval._get_feature_names(\n",
    "                global_fold_classifier._inner\n",
    "            )\n",
    "            if global_fold_feature_importances is None:\n",
    "                logger.warning(\n",
    "                    f\"No feature importances available for multiclass {model_name} (global fold)\"\n",
    "                )\n",
    "                # skip this model\n",
    "                return\n",
    "            global_fold_feature_importances = pd.DataFrame(\n",
    "                global_fold_feature_importances,\n",
    "                index=global_fold_classifier.classes_,\n",
    "                columns=global_fold_feature_names,\n",
    "            )\n",
    "            global_fold_feature_importances.to_csv(\n",
    "                f\"{highres_results_output_prefix}.feature_importances.{model_name}.raw_coefs.global_fold.tsv\",\n",
    "                sep=\"\\t\",\n",
    "            )\n",
    "            for fig_name, fig in plot_multiclass_feature_importances(\n",
    "                model_name=model_name,\n",
    "                raw_coefs_mean=global_fold_feature_importances,\n",
    "                raw_coefs_std=None,\n",
    "                gene_locus=gene_locus,\n",
    "                target_obs_column=target_obs_column,\n",
    "                metamodel_flavor=metamodel_flavor,\n",
    "                n_folds=1,\n",
    "            ):\n",
    "                fname = f\"{highres_results_output_prefix}.feature_importances_global_fold.{model_name}.{fig_name}.png\"\n",
    "                logger.debug(f\"{fig_name} -> {fname}\")\n",
    "                genetools.plots.savefig(\n",
    "                    fig,\n",
    "                    fname,\n",
    "                    dpi=300,\n",
    "                )\n",
    "                plt.close(fig)\n",
    "    else:\n",
    "        logger.warning(\n",
    "            f\"Feature importances not plotted for {model_name}: not a recognized tree or linear model.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def compute_pairwise_scores(\n",
    "    experiment_set_global_performance: crosseval.ExperimentSetGlobalPerformance,\n",
    "    models_of_interest: List[str],\n",
    ") -> Generator[Tuple[str, pd.DataFrame, plt.Figure, plt.Axes], None, None]:\n",
    "    \"\"\"\n",
    "    Given a ExperimentSetGlobalPerformance, compute pairwise scores for all models of interest.\n",
    "    Returns generator yielding (model_name, table, fig, ax) tuples.\n",
    "    \"\"\"\n",
    "\n",
    "    def pairwise_roc_auc_score_function_factory(classA, classB):\n",
    "        \"\"\"Generate a ROC AUC score function for classA vs classB.\"\"\"\n",
    "\n",
    "        def score_func(\n",
    "            y_true: np.ndarray,\n",
    "            y_score: np.ndarray,\n",
    "            labels: Optional[np.ndarray] = None,\n",
    "            **kwargs,\n",
    "        ):\n",
    "            # Emulate how multiclass OvO score works:\n",
    "            # https://github.com/scikit-learn/scikit-learn/blob/00032b09c7feea08edd4486c522c2d962f9d52ec/sklearn/metrics/_base.py#L132\n",
    "\n",
    "            # y_true may have different entries than y_score/labels\n",
    "            # so start by extending y_score/labels to match y_true, as we eventually will do in roc_auc_score anyway.\n",
    "            # otherwise the column_indices lookups below will fail.\n",
    "            y_score = np.array(y_score)  # defensive cast from pandas dataframe\n",
    "            y_score, labels = multiclass_metrics._inject_missing_labels(\n",
    "                y_true=y_true, y_score=y_score, labels=labels\n",
    "            )\n",
    "\n",
    "            # defensive cast; otherwise np.argwhere fails in column_indices lookup below.\n",
    "            labels = np.array(labels)\n",
    "\n",
    "            # Subset to entries where y_true belongs to one of the classes:\n",
    "            mask = np.logical_or(y_true == classA, y_true == classB)\n",
    "            y_true = y_true[mask]\n",
    "            y_score = y_score[mask, :]\n",
    "\n",
    "            # Subset to those columns:\n",
    "            new_label_order = np.array([classA, classB])\n",
    "            column_indices = np.array(\n",
    "                [\n",
    "                    np.argwhere(labels == classA)[0][0],\n",
    "                    np.argwhere(labels == classB)[0][0],\n",
    "                ]\n",
    "            )\n",
    "            y_score = y_score[:, column_indices]\n",
    "\n",
    "            # Run ROC AUC score with modified input data\n",
    "            return multiclass_metrics.roc_auc_score(\n",
    "                y_true=y_true,\n",
    "                y_score=y_score,\n",
    "                labels=new_label_order,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        return score_func\n",
    "\n",
    "    # Get class names.\n",
    "    # To do so, we first need to extract a model global performance object\n",
    "    a_model_global_performance = next(\n",
    "        iter(experiment_set_global_performance.model_global_performances.values())\n",
    "    )\n",
    "    # Now get class names (minus unknown/abstain label)\n",
    "    classes = set(a_model_global_performance.confusion_matrix_label_ordering) - {\n",
    "        a_model_global_performance.abstain_label\n",
    "    }\n",
    "    classes = sorted(classes)\n",
    "\n",
    "    # Create a dictionary of probability scorers:\n",
    "    # Treat ROC for every pair of classes as a separate metric.\n",
    "    # Each metric has a score function. pairwise_roc_auc_score_function_factory generates a score function that extracts test examples for a specific pair of classes and computes the AUC.\n",
    "    probability_scorers = {}\n",
    "    friendly_metric_name_subcomponents = {}\n",
    "    for classA, classB in itertools.combinations(classes, 2):\n",
    "        friendly_name = f\"ROC-AUC {classA} vs {classB}\"\n",
    "        probability_scorers[f\"rocauc:{classA}|{classB}\"] = (\n",
    "            pairwise_roc_auc_score_function_factory(classA, classB),\n",
    "            friendly_name,\n",
    "            {},\n",
    "        )\n",
    "        # Record the two classes that have been stitched into each friendly metric name.\n",
    "        # The metric name will have an additional suffix added to it when summarized, so let's include that here:\n",
    "        friendly_metric_name_subcomponents[f\"{friendly_name} per fold\"] = (\n",
    "            classA,\n",
    "            classB,\n",
    "        )\n",
    "\n",
    "    # Here is how we could run metrics for a single model:\n",
    "    # a_model_global_performance.aggregated_per_fold_scores(\n",
    "    #     with_abstention=False, label_scorers={}, probability_scorers=probability_scorers\n",
    "    # )\n",
    "\n",
    "    # Run all these pairwise metrics on all the models\n",
    "    combined_stats = experiment_set_global_performance.get_model_comparison_stats(\n",
    "        probability_scorers=probability_scorers, label_scorers={}\n",
    "    )\n",
    "    # Isolate columns corresponding to our desired ROC AUC OvO metrics, and rename them back to the source components (pair of class names)\n",
    "    combined_stats = combined_stats[friendly_metric_name_subcomponents.keys()].rename(\n",
    "        columns=friendly_metric_name_subcomponents\n",
    "    )\n",
    "\n",
    "    for model_name in models_of_interest:\n",
    "        # For a particular model, create a triangular table of pairwise OvO ROC AUCs\n",
    "        model_stats = combined_stats.loc[model_name].rename(\"ROC AUC\").to_frame()\n",
    "        model_stats.index = pd.MultiIndex.from_tuples(\n",
    "            model_stats.index, names=(\"classA\", \"classB\")\n",
    "        )\n",
    "\n",
    "        # Create a triangular table of pairwise OvO ROC AUCs with all cross validation data.\n",
    "        triangular_table = (\n",
    "            model_stats.reset_index()\n",
    "            .pivot(index=\"classA\", columns=\"classB\", values=\"ROC AUC\")\n",
    "            .fillna(\"\")\n",
    "            .T\n",
    "        )\n",
    "\n",
    "        # Plot a triangular heatmap with just the mean pairwise ROC AUCs across cross validation folds.\n",
    "        plot_df = (\n",
    "            model_stats[\"ROC AUC\"]\n",
    "            # Remove +/- stddev across cross validation folds\n",
    "            .str.split(\" +/-\", regex=False)\n",
    "            .str[0]\n",
    "            .astype(float)\n",
    "            .reset_index()\n",
    "            .pivot(index=\"classA\", columns=\"classB\", values=\"ROC AUC\")\n",
    "            .T\n",
    "            # add each class vs itself (a diagonal of NaNs)\n",
    "            .reindex(index=classes, columns=classes)\n",
    "        )\n",
    "        fig, ax = plot_triangular_heatmap(\n",
    "            plot_df,\n",
    "            colorbar_label=\"ROC AUC\",\n",
    "            figsize=(5, 5),\n",
    "            # hardcode vmin and vmax so color scales are consistent across metamodel flavors\n",
    "            vmin=0.8,\n",
    "            vmax=1.0,\n",
    "        )\n",
    "        genetools.plots.wrap_tick_labels(\n",
    "            ax, wrap_x_axis=True, wrap_y_axis=True, wrap_amount=10\n",
    "        )\n",
    "        yield model_name, triangular_table, fig, ax\n",
    "\n",
    "\n",
    "def compute_per_class_scores(\n",
    "    experiment_set_global_performance: crosseval.ExperimentSetGlobalPerformance,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Given a ExperimentSetGlobalPerformance, compute per-class ROC AUC scores (OvR).\n",
    "    Returns DataFrame with:\n",
    "        rows = model names,\n",
    "        columns = class names,\n",
    "        values = OvR ROC AUC mean and standard deviation across cross validation folds.\n",
    "    \"\"\"\n",
    "\n",
    "    def per_class_ovr_roc_auc_score_function_factory(class_name):\n",
    "        \"\"\"Generate an OvR ROC AUC score function for a particular class.\"\"\"\n",
    "\n",
    "        def score_func(\n",
    "            y_true: np.ndarray,\n",
    "            y_score: np.ndarray,\n",
    "            labels: np.ndarray,\n",
    "            **kwargs,\n",
    "        ):\n",
    "            # one-vs-rest AUCs for each class (due to average=None and multi_class=\"ovr\")\n",
    "            # first, run the inject_missing_labels functionality that happens inside crosseval_scores.roc_auc_score,\n",
    "            # so we can create the Series correctly.\n",
    "            labels_with_any_missing_added = multiclass_metrics._inject_missing_labels(\n",
    "                y_true=y_true, y_score=y_score, labels=labels\n",
    "            )[1]\n",
    "            aucs = pd.Series(\n",
    "                multiclass_metrics.roc_auc_score(\n",
    "                    y_true=y_true,\n",
    "                    y_score=y_score,\n",
    "                    labels=labels,\n",
    "                    average=None,\n",
    "                    multi_class=\"ovr\",\n",
    "                    **kwargs,\n",
    "                ),\n",
    "                index=labels_with_any_missing_added,\n",
    "            )\n",
    "            return aucs[class_name]\n",
    "\n",
    "        return score_func\n",
    "\n",
    "    # Get class names.\n",
    "    # To do so, we first need to extract a model global performance object\n",
    "    a_model_global_performance = next(\n",
    "        iter(experiment_set_global_performance.model_global_performances.values())\n",
    "    )\n",
    "    # Now get class names (minus unknown/abstain label)\n",
    "    classes = set(a_model_global_performance.confusion_matrix_label_ordering) - {\n",
    "        a_model_global_performance.abstain_label\n",
    "    }\n",
    "\n",
    "    # Create a dictionary of probability scorers:\n",
    "    # Treat ROC for each class as a separate metric.\n",
    "    # Each metric has a score function.\n",
    "    # per_class_ovr_roc_auc_score_function_factory generates a score function that computes OvR scores for all classes and then returns the one for the desired class.\n",
    "    probability_scorers = {}\n",
    "    map_friendly_metric_name_to_class_name = {}\n",
    "    for class_name in classes:\n",
    "        friendly_name = f\"ROC-AUC {class_name} OvR\"\n",
    "        probability_scorers[f\"rocauc:{class_name}\"] = (\n",
    "            per_class_ovr_roc_auc_score_function_factory(class_name),\n",
    "            friendly_name,\n",
    "            {},\n",
    "        )\n",
    "        # Record the two classes that have been stitched into each friendly metric name.\n",
    "        # The metric name will have an additional suffix added to it when summarized, so let's include that here:\n",
    "        map_friendly_metric_name_to_class_name[f\"{friendly_name} per fold\"] = class_name\n",
    "\n",
    "    # Here is how we could run metrics for a single model:\n",
    "    #     a_model_global_performance.aggregated_per_fold_scores(\n",
    "    #         with_abstention=False,\n",
    "    #         label_scorers={},\n",
    "    #         probability_scorers=probability_scorers,\n",
    "    #     )\n",
    "\n",
    "    # Run all the metrics on all the models\n",
    "    per_class_stats = experiment_set_global_performance.get_model_comparison_stats(\n",
    "        probability_scorers=probability_scorers, label_scorers={}\n",
    "    )\n",
    "    # Isolate columns corresponding to our desired ROC AUC OvR metrics, and rename them back to the source class name\n",
    "    # Use a reindex instead of column subset in case any of the keys are missing (e.g. 'ROC-AUC HIV OvR per fold')\n",
    "    per_class_stats = per_class_stats.reindex(\n",
    "        columns=list(map_friendly_metric_name_to_class_name.keys()), fill_value=np.nan\n",
    "    ).rename(columns=map_friendly_metric_name_to_class_name)\n",
    "\n",
    "    # Now each column is the OvR ROC AUC for a particular class vs all others (column name = class name),\n",
    "    # and each row corresponds to one model.\n",
    "\n",
    "    # Also generate the usual metrics\n",
    "    typical_stats = experiment_set_global_performance.get_model_comparison_stats()\n",
    "\n",
    "    return per_class_stats, typical_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfd66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_train_fold_name = \"train_smaller\"\n",
    "metamodel_fold_label_train = \"validation\"\n",
    "\n",
    "\n",
    "def analyze_metamodel_flavor(\n",
    "    gene_locus: GeneLocus,\n",
    "    target_obs_column: TargetObsColumnEnum,\n",
    "    metamodel_flavor: str,\n",
    "    metamodel_config: MetamodelConfig,\n",
    "    forced_abstentions: Optional[List[str]] = None,\n",
    ") -> Optional[List[str]]:\n",
    "    # should already exist:\n",
    "    metamodels_base_dir = BlendingMetamodel._get_metamodel_base_dir(\n",
    "        gene_locus=gene_locus,\n",
    "        target_obs_column=target_obs_column,\n",
    "        metamodel_flavor=metamodel_flavor,\n",
    "    )\n",
    "\n",
    "    _output_suffix = Path(gene_locus.name) / target_obs_column.name / metamodel_flavor\n",
    "    # might not exist yet:\n",
    "    output_base_dir = (\n",
    "        config.paths.second_stage_blending_metamodel_output_dir / _output_suffix\n",
    "    )\n",
    "    highres_output_base_dir = (\n",
    "        config.paths.high_res_outputs_dir / \"metamodel\" / _output_suffix\n",
    "    )\n",
    "    output_base_dir.mkdir(parents=True, exist_ok=True)\n",
    "    highres_output_base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fname_prefix = (\n",
    "        f\"{base_model_train_fold_name}_applied_to_{metamodel_fold_label_train}_model\"\n",
    "    )\n",
    "    model_prefix = metamodels_base_dir / fname_prefix\n",
    "    results_output_prefix = output_base_dir / fname_prefix\n",
    "    highres_results_output_prefix = highres_output_base_dir / fname_prefix\n",
    "\n",
    "    computed_abstentions = None\n",
    "\n",
    "    try:\n",
    "        # Load and summarize\n",
    "        experiment_set = crosseval.ExperimentSet.load_from_disk(\n",
    "            output_prefix=model_prefix\n",
    "        )\n",
    "\n",
    "        # Note that default y_true from BlendingMetamodel._featurize() is target_obs_column.value.blended_evaluation_column_name\n",
    "        # Use DROP_INCOMPLETE_FOLDS setting because alternate classification targets might not be well-split in the small validation set of the cross-validation folds that were designed to stratify disease.\n",
    "        # In the cases of some classification targets, we might need to automatically drop folds that have only a single class in the metamodel training data (i.e. in the validation set).\n",
    "        experiment_set_global_performance = experiment_set.summarize(\n",
    "            remove_incomplete_strategy=crosseval.RemoveIncompleteStrategy.DROP_INCOMPLETE_FOLDS\n",
    "        )\n",
    "        experiment_set_global_performance.export_all_models(\n",
    "            func_generate_classification_report_fname=lambda model_name: f\"{results_output_prefix}.classification_report.test_set_performance.{model_name}.txt\",\n",
    "            func_generate_confusion_matrix_fname=lambda model_name: f\"{results_output_prefix}.confusion_matrix.test_set_performance.{model_name}.png\",\n",
    "            dpi=300,\n",
    "        )\n",
    "        combined_stats = experiment_set_global_performance.get_model_comparison_stats()\n",
    "        combined_stats.to_csv(\n",
    "            f\"{results_output_prefix}.compare_model_scores.test_set_performance.tsv\",\n",
    "            sep=\"\\t\",\n",
    "        )\n",
    "        display(\n",
    "            Markdown(\n",
    "                f\"## {gene_locus}, {target_obs_column}, metamodel flavor {metamodel_flavor} from {model_prefix} to {results_output_prefix}\"\n",
    "            )\n",
    "        )\n",
    "        print(metamodel_config)\n",
    "        display(combined_stats)\n",
    "\n",
    "        # Extract list of abstentions.\n",
    "        # (Abstentions should be the same for all models in this metamodel flavor,\n",
    "        # because they are all trained from the same featurization which determines the abstentions.)\n",
    "        any_model_global_performance = next(\n",
    "            iter(experiment_set_global_performance.model_global_performances.values())\n",
    "        )\n",
    "        computed_abstentions = (\n",
    "            any_model_global_performance.cv_abstentions_metadata[\n",
    "                \"specimen_label\"\n",
    "            ].tolist()\n",
    "            if any_model_global_performance.cv_abstentions_metadata is not None\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        # Redo, but (potentially) override y_true to pass in e.g. disease with past exposures separated out (delinates past exposures on ground truth axis)\n",
    "        # For cleaner confusion matrices\n",
    "        # (But this changes global score metrics)\n",
    "        experiment_set.summarize(\n",
    "            global_evaluation_column_name=target_obs_column.value.confusion_matrix_expanded_column_name,\n",
    "            remove_incomplete_strategy=crosseval.RemoveIncompleteStrategy.DROP_INCOMPLETE_FOLDS,\n",
    "        ).export_all_models(\n",
    "            func_generate_classification_report_fname=lambda model_name: f\"{highres_results_output_prefix}.classification_report.test_set_performance.{model_name}.expanded_confusion_matrix.txt\",\n",
    "            func_generate_confusion_matrix_fname=lambda model_name: f\"{highres_results_output_prefix}.confusion_matrix.test_set_performance.{model_name}.expanded_confusion_matrix.png\",\n",
    "            confusion_matrix_true_label=\"Patient of origin - expanded\",\n",
    "            dpi=300,\n",
    "        )\n",
    "\n",
    "        # Do extra analyses for the default classification target for this cross validation split strategy:\n",
    "        if (\n",
    "            target_obs_column\n",
    "            == map_cross_validation_split_strategy_to_default_target_obs_column[\n",
    "                config.cross_validation_split_strategy\n",
    "            ]\n",
    "        ):\n",
    "            if target_obs_column == TargetObsColumnEnum.disease:\n",
    "                # Special case when default classification target is \"disease\":\n",
    "                # Redo, but (potentially) override y_true to pass in \"disease_subtype\" for ground truth axis\n",
    "                # (But this changes global score metrics)\n",
    "                experiment_set.summarize(\n",
    "                    global_evaluation_column_name=\"disease_subtype\",\n",
    "                    remove_incomplete_strategy=crosseval.RemoveIncompleteStrategy.DROP_INCOMPLETE_FOLDS,\n",
    "                ).export_all_models(\n",
    "                    func_generate_classification_report_fname=lambda model_name: f\"{highres_results_output_prefix}.classification_report.test_set_performance.{model_name}.expanded_confusion_matrix_disease_subtype.txt\",\n",
    "                    func_generate_confusion_matrix_fname=lambda model_name: f\"{highres_results_output_prefix}.confusion_matrix.test_set_performance.{model_name}.expanded_confusion_matrix_disease_subtype.png\",\n",
    "                    confusion_matrix_true_label=\"Patient of origin - subtype\",\n",
    "                    dpi=300,\n",
    "                )\n",
    "\n",
    "            # Also resummarize by a combined variable of disease + ethnicity (or other default classification target + ethnicity)\n",
    "            # But first, fillna on ethnicity column to change nans to \"Unknown\"\n",
    "            experiment_set_modified_ethnicity_metadata_column = (\n",
    "                # Create a copy of the experiment_set, to not disturb original metadata dataframes\n",
    "                experiment_set.copy()\n",
    "            )\n",
    "            for (\n",
    "                model_single_fold_performance\n",
    "            ) in (\n",
    "                experiment_set_modified_ethnicity_metadata_column.model_outputs.values()\n",
    "            ):\n",
    "                # Modify every model_single_fold_performance's metadata: fillna on the ethnicity_condensed column\n",
    "                for df in [\n",
    "                    model_single_fold_performance.test_metadata,\n",
    "                    model_single_fold_performance.test_abstention_metadata,\n",
    "                ]:\n",
    "                    if df is None or df.shape[0] == 0:\n",
    "                        continue\n",
    "                    df[\"ethnicity_condensed\"].fillna(\"Unknown\", inplace=True)\n",
    "            experiment_set_modified_ethnicity_metadata_column.summarize(\n",
    "                global_evaluation_column_name=[\n",
    "                    crosseval.Y_TRUE_VALUES,\n",
    "                    \"ethnicity_condensed\",\n",
    "                ],\n",
    "                remove_incomplete_strategy=crosseval.RemoveIncompleteStrategy.DROP_INCOMPLETE_FOLDS,\n",
    "            ).export_all_models(\n",
    "                func_generate_classification_report_fname=lambda model_name: f\"{highres_results_output_prefix}.classification_report.test_set_performance.{model_name}.expanded_confusion_matrix_ethnicity_condensed.txt\",\n",
    "                func_generate_confusion_matrix_fname=lambda model_name: f\"{highres_results_output_prefix}.confusion_matrix.test_set_performance.{model_name}.expanded_confusion_matrix_ethnicity_condensed.png\",\n",
    "                confusion_matrix_true_label=\"Patient of origin - ancestry\",\n",
    "                dpi=300,\n",
    "            )\n",
    "\n",
    "            # Also resummarize by a combined variable of disease + age_group_pediatric (or other default classification target + age_group_pediatric)\n",
    "            experiment_set_modified_metadata_age_pediatric_column = (\n",
    "                # Create a copy of the experiment_set, to not disturb original metadata dataframes\n",
    "                experiment_set.copy()\n",
    "            )\n",
    "            for (\n",
    "                model_single_fold_performance\n",
    "            ) in (\n",
    "                experiment_set_modified_metadata_age_pediatric_column.model_outputs.values()\n",
    "            ):\n",
    "                # Modify every model_single_fold_performance's metadata: create age_group_pediatric column\n",
    "                def _mutate_metadata_for_age_pediatric_analysis(df):\n",
    "                    if df is None or df.shape[0] == 0:\n",
    "                        # Skip empty dataframes\n",
    "                        # This can happen if e.g. there are no abstentions. Then test_abstention_metadata will be empty.\n",
    "                        return df\n",
    "\n",
    "                    # Enrich metadata to include the age_group_pediatric computed column.\n",
    "                    # So far only \"age\" column exists in the model outputs saved to disk by metamodel train.\n",
    "                    # Note this returns a copy instead of mutating in place.\n",
    "                    df = helpers.enrich_metadata(df)\n",
    "\n",
    "                    # Fill NaNs intelligently:\n",
    "                    if (df[\"data_source\"] == DataSource.in_house).all():\n",
    "                        # If in-house data:\n",
    "                        # We know we have very few children cohorts and they are clearly indicated in the study name.\n",
    "                        # If study name indicates that this is a pediatric cohort, set to \"under 18\". Otherwise set to 18+.\n",
    "                        slice_children = df[\"study_name\"].str.contains(\n",
    "                            \"pediatric|children\", regex=True, case=False\n",
    "                        )\n",
    "                        df.loc[slice_children, \"age_group_pediatric\"] = df.loc[\n",
    "                            slice_children, \"age_group_pediatric\"\n",
    "                        ].fillna(\"under 18\")\n",
    "                        df[\"age_group_pediatric\"].fillna(\"18+\", inplace=True)\n",
    "                    else:\n",
    "                        df[\"age_group_pediatric\"].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "                    return df\n",
    "\n",
    "                # Apply mutations to each dataframe and update the model_single_fold_performance\n",
    "                # We wrote it this way rather than a loop to make sure we assign back to the original dataframe objects, so the changes are picked up below.\n",
    "                model_single_fold_performance.test_metadata = (\n",
    "                    _mutate_metadata_for_age_pediatric_analysis(\n",
    "                        model_single_fold_performance.test_metadata\n",
    "                    )\n",
    "                )\n",
    "                model_single_fold_performance.test_abstention_metadata = (\n",
    "                    _mutate_metadata_for_age_pediatric_analysis(\n",
    "                        model_single_fold_performance.test_abstention_metadata\n",
    "                    )\n",
    "                )\n",
    "            # Now make new confusion matrices with these metadata changes included\n",
    "            experiment_set_modified_metadata_age_pediatric_column.summarize(\n",
    "                global_evaluation_column_name=[\n",
    "                    crosseval.Y_TRUE_VALUES,\n",
    "                    \"age_group_pediatric\",\n",
    "                ],\n",
    "                remove_incomplete_strategy=crosseval.RemoveIncompleteStrategy.DROP_INCOMPLETE_FOLDS,\n",
    "            ).export_all_models(\n",
    "                func_generate_classification_report_fname=lambda model_name: f\"{highres_results_output_prefix}.classification_report.test_set_performance.{model_name}.expanded_confusion_matrix_age_group_pediatric.txt\",\n",
    "                func_generate_confusion_matrix_fname=lambda model_name: f\"{highres_results_output_prefix}.confusion_matrix.test_set_performance.{model_name}.expanded_confusion_matrix_age_group_pediatric.png\",\n",
    "                confusion_matrix_true_label=\"Patient of origin - pediatric vs adult\",\n",
    "                dpi=300,\n",
    "            )\n",
    "\n",
    "            # Calculate ROC AUC scores for each class in One vs Rest fashion.\n",
    "            # To be consistent across metamodel flavors, so we can compare scores apples-to-apples,\n",
    "            # we will force abstention on samples in the forced_abstentions list,\n",
    "            # which is computed for the \"default\" flavor and then passed around.\n",
    "            experiment_set_summary_with_forced_abstentions_applied = crosseval.ExperimentSet(\n",
    "                model_outputs={\n",
    "                    key: model_single_fold_performance.apply_abstention_mask(\n",
    "                        model_single_fold_performance.test_metadata.index.isin(\n",
    "                            forced_abstentions if forced_abstentions is not None else []\n",
    "                        )\n",
    "                    )\n",
    "                    for key, model_single_fold_performance in experiment_set.model_outputs.items()\n",
    "                }\n",
    "            ).summarize()\n",
    "            (\n",
    "                roc_auc_ovr_per_class_scores,\n",
    "                standard_scores_with_forced_abstentions,\n",
    "            ) = compute_per_class_scores(\n",
    "                experiment_set_summary_with_forced_abstentions_applied\n",
    "            )\n",
    "            # Export the OvR per-class scores and the standard scores\n",
    "            roc_auc_ovr_per_class_scores.to_csv(\n",
    "                f\"{highres_results_output_prefix}.per_class_roc_auc_scores_ovr.tsv\",\n",
    "                sep=\"\\t\",\n",
    "            )\n",
    "            standard_scores_with_forced_abstentions.to_csv(\n",
    "                f\"{highres_results_output_prefix}.standard_scores_with_forced_consistent_abstentions.tsv\",\n",
    "                sep=\"\\t\",\n",
    "            )\n",
    "\n",
    "            # Calculate pairwise OvO ROC AUC scores for all pairs of classes (i.e. one class vs another class)\n",
    "            for (\n",
    "                model_name,\n",
    "                triangular_table,\n",
    "                triangular_fig,\n",
    "                triangular_ax,\n",
    "            ) in compute_pairwise_scores(\n",
    "                experiment_set_global_performance=experiment_set_global_performance,\n",
    "                models_of_interest=config.model_names_to_train,\n",
    "            ):\n",
    "                triangular_ax.set_title(\n",
    "                    f\"\"\"{gene_locus}\n",
    "{target_obs_column}\n",
    "\n",
    "metamodel flavor: {metamodel_flavor}\n",
    "model: {model_name}\"\"\"\n",
    "                )\n",
    "                # Export\n",
    "                genetools.plots.savefig(\n",
    "                    triangular_fig,\n",
    "                    f\"{highres_results_output_prefix}.pairwise_roc_auc_scores.{model_name}.png\",\n",
    "                    dpi=300,\n",
    "                )\n",
    "                triangular_table.to_csv(\n",
    "                    f\"{highres_results_output_prefix}.pairwise_roc_auc_scores.{model_name}.tsv\",\n",
    "                    sep=\"\\t\",\n",
    "                )\n",
    "                plt.close(triangular_fig)\n",
    "\n",
    "        for (\n",
    "            model_name,\n",
    "            model_global_performance,\n",
    "        ) in experiment_set_global_performance.model_global_performances.items():\n",
    "            if model_name not in config.model_names_to_analyze_extra:\n",
    "                # Skip further analysis for most models\n",
    "                continue\n",
    "\n",
    "            # review classification for each specimen\n",
    "            individual_classifications = model_global_performance.get_all_entries()\n",
    "            individual_classifications.to_csv(\n",
    "                f\"{highres_results_output_prefix}.classification_raw_per_specimen.test_set_performance.{model_name}.with_abstention.tsv\",\n",
    "                sep=\"\\t\",\n",
    "                index=None,\n",
    "            )\n",
    "\n",
    "            # filter to mistakes (including abstentions)\n",
    "            mistakes = individual_classifications[\n",
    "                individual_classifications[\"y_true\"]\n",
    "                != individual_classifications[\"y_pred\"]\n",
    "            ]\n",
    "            mistakes.to_csv(\n",
    "                f\"{highres_results_output_prefix}.classification_errors.test_set_performance.{model_name}.with_abstention.tsv\",\n",
    "                sep=\"\\t\",\n",
    "                index=None,\n",
    "            )\n",
    "\n",
    "            # filter further to abstentions\n",
    "            abstentions = individual_classifications[\n",
    "                individual_classifications[\"y_pred\"]\n",
    "                == model_global_performance.abstain_label\n",
    "            ]\n",
    "            abstentions.to_csv(\n",
    "                f\"{highres_results_output_prefix}.classification_abstentions.test_set_performance.{model_name}.with_abstention.tsv\",\n",
    "                sep=\"\\t\",\n",
    "                index=None,\n",
    "            )\n",
    "\n",
    "            # label correct/incorrect\n",
    "            individual_classifications[\"classification_success\"] = \"Correct\"\n",
    "            where_wrong = (\n",
    "                individual_classifications[\"y_true\"]\n",
    "                != individual_classifications[\"y_pred\"]\n",
    "            )\n",
    "            individual_classifications.loc[\n",
    "                where_wrong, \"classification_success\"\n",
    "            ] = \"Incorrect\"\n",
    "\n",
    "            # Plot difference between top two predicted probabilities, p1 - p2,\n",
    "            # and difference in logits (log odds) of the top two classes, log(p1/(1-p1)) - log(p2/(1-p2)),\n",
    "            # to account for the fact that these are probability distributions that sum to 1.\n",
    "            # (That's the natural log, i.e. log base e.)\n",
    "            # Alternative considered: difference in log probabilities of top two classes, i.e. log(p1) - log(p2), but that won't distinguish cases like p1=0.5, p2=0.25 from p1=0.4, p2=0.2.\n",
    "            # difference_between_top_two_predicted_probas was already generated, but we can create the rest ourselves here.\n",
    "            # TODO: consider other metrics from https://robertmunro.com/uncertainty_sampling_example.html?\n",
    "            p1, p2 = (\n",
    "                individual_classifications[\"max_predicted_proba\"],\n",
    "                individual_classifications[\"second_highest_predicted_proba\"],\n",
    "            )\n",
    "            epsilon = 1e-8  # avoid log(0) if p=0 or p=1\n",
    "            individual_classifications[\n",
    "                \"difference_between_logits_of_top_two_classes\"\n",
    "            ] = (np.log(p1 + epsilon) - np.log(1 - p1 + epsilon)) - (\n",
    "                np.log(p2 + epsilon) - np.log(1 - p2 + epsilon)\n",
    "            )\n",
    "            order = [\"Incorrect\", \"Correct\"]\n",
    "            for metric, label in [\n",
    "                (\n",
    "                    \"difference_between_top_two_predicted_probas\",\n",
    "                    \"Difference between\\ntop two predicted probabilities\",\n",
    "                ),\n",
    "                (\n",
    "                    \"difference_between_logits_of_top_two_classes\",\n",
    "                    \"Difference between log odds\\nof top two predicted classes\",\n",
    "                ),\n",
    "            ]:\n",
    "                # Subset order to actually present values.\n",
    "                # Avoids seaborn boxplot error (we believe this is v0.13+ only): \"Missing x value(s) `\"Incorrect\"` in classification_success (specified in `order`)\"\n",
    "                filtered_order = [\n",
    "                    x\n",
    "                    for x in order\n",
    "                    if x in individual_classifications[\"classification_success\"].values\n",
    "                ]\n",
    "                fig = plt.figure(figsize=(3, 5))\n",
    "                sns.boxplot(\n",
    "                    data=individual_classifications,\n",
    "                    x=\"classification_success\",\n",
    "                    y=metric,\n",
    "                    # TODO(seaborn >= 0.13): reenable hue and legend:\n",
    "                    # hue=\"classification_success\",\n",
    "                    # legend=False,\n",
    "                    order=filtered_order,\n",
    "                    palette=sns.color_palette(\"Paired\")[:2],\n",
    "                )\n",
    "                plt.title(f\"Blending metamodel {model_name}\")\n",
    "                plt.xlabel(\"Specimen classification\")\n",
    "                plt.ylabel(label)\n",
    "                sns.despine()\n",
    "                genetools.plots.savefig(\n",
    "                    fig,\n",
    "                    f\"{highres_results_output_prefix}.errors_versus_{metric}.test_set_performance.{model_name}.with_abstention.vertical.simple.png\",\n",
    "                    dpi=300,\n",
    "                )\n",
    "                plt.close(fig)\n",
    "\n",
    "                ##\n",
    "\n",
    "                # Repeat the plot, but now with scatter points above the boxplot, and with statistical test.\n",
    "                fig = plt.figure(figsize=(3, 5))\n",
    "                ax = sns.boxplot(\n",
    "                    data=individual_classifications,\n",
    "                    x=\"classification_success\",\n",
    "                    y=metric,\n",
    "                    # TODO(seaborn >= 0.13): reenable hue and legend:\n",
    "                    # hue=\"classification_success\",\n",
    "                    # legend=False,\n",
    "                    order=filtered_order,\n",
    "                    palette=sns.color_palette(\"Paired\")[:2],\n",
    "                    # Disable outlier markers:\n",
    "                    fliersize=0,\n",
    "                    zorder=20,\n",
    "                )\n",
    "\n",
    "                for patch in ax.patches:\n",
    "                    # Set boxplot alpha transparency: https://github.com/mwaskom/seaborn/issues/979#issuecomment-1144615001\n",
    "                    r, g, b, a = patch.get_facecolor()\n",
    "                    patch.set_facecolor((r, g, b, 0.3))\n",
    "                sns.stripplot(\n",
    "                    data=individual_classifications,\n",
    "                    x=\"classification_success\",\n",
    "                    y=metric,\n",
    "                    order=filtered_order,\n",
    "                    hue=\"classification_success\",\n",
    "                    legend=None,\n",
    "                    linewidth=0.25,\n",
    "                    edgecolor=\"gray\",\n",
    "                    palette=sns.color_palette(\"Paired\")[:2],\n",
    "                    ax=ax,\n",
    "                    size=2.5,\n",
    "                    jitter=0.25,\n",
    "                    zorder=1,\n",
    "                    alpha=0.7,\n",
    "                )\n",
    "\n",
    "                # Annotate with statistical significance\n",
    "                # One sided test for Incorrect having *lower* scores than Correct\n",
    "                test_results = None\n",
    "                if len(filtered_order) > 1:\n",
    "                    annot = Annotator(\n",
    "                        ax=ax,\n",
    "                        pairs=[filtered_order],\n",
    "                        data=individual_classifications,\n",
    "                        x=\"classification_success\",\n",
    "                        y=metric,\n",
    "                        order=filtered_order,\n",
    "                    )\n",
    "                    annot.configure(\n",
    "                        test=\"Mann-Whitney-ls\",\n",
    "                        text_format=\"star\",\n",
    "                        loc=\"outside\",\n",
    "                        verbose=2,\n",
    "                    )\n",
    "                    annot.apply_test(method=\"asymptotic\")\n",
    "                    ax, test_results = annot.annotate()\n",
    "\n",
    "                ax.set_xticklabels(\n",
    "                    genetools.plots.add_sample_size_to_labels(\n",
    "                        labels=ax.get_xticklabels(),\n",
    "                        data=individual_classifications,\n",
    "                        hue_key=\"classification_success\",\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                plt.xlabel(f\"Sample classification\\n{model_name}\")\n",
    "                plt.ylabel(label)\n",
    "                sns.despine(ax=ax)\n",
    "                plt.tight_layout()\n",
    "                genetools.plots.savefig(\n",
    "                    fig,\n",
    "                    f\"{highres_results_output_prefix}.errors_versus_{metric}.test_set_performance.{model_name}.with_abstention.vertical.png\",\n",
    "                    dpi=300,\n",
    "                )\n",
    "                genetools.plots.savefig(\n",
    "                    fig,\n",
    "                    f\"{highres_results_output_prefix}.errors_versus_{metric}.test_set_performance.{model_name}.with_abstention.vertical.pdf\",\n",
    "                    dpi=600,\n",
    "                )\n",
    "                to_dump = dict(\n",
    "                    individual_classifications=individual_classifications,\n",
    "                    metric=metric,\n",
    "                    label=label,\n",
    "                    filtered_order=filtered_order,\n",
    "                    model_name=model_name,\n",
    "                    test_results=test_results,\n",
    "                    plot_fname=f\"{highres_results_output_prefix}.errors_versus_{metric}.test_set_performance.{model_name}.with_abstention.vertical.png\",\n",
    "                )\n",
    "                joblib.dump(\n",
    "                    to_dump,\n",
    "                    f\"{highres_results_output_prefix}.errors_versus_{metric}.test_set_performance.{model_name}.with_abstention.vertical.plot_components.joblib\",\n",
    "                )\n",
    "\n",
    "                if test_results is not None and len(test_results) > 0:\n",
    "                    with open(\n",
    "                        f\"{highres_results_output_prefix}.errors_versus_{metric}.test_set_performance.{model_name}.with_abstention.vertical.test_results.txt\",\n",
    "                        \"w\",\n",
    "                    ) as f:\n",
    "                        f.write(test_results[0].data.formatted_output)\n",
    "                plt.close(fig)\n",
    "\n",
    "            try:\n",
    "                # Try to load global fold classifier for analysis, too.\n",
    "                # It wasn't included in the ExperimentSet, because no .metadata_joblib was generated, since the global fold does not have a test set.\n",
    "                # Note that this will only process global fold classifiers for models that were trained for at least one cross validation fold.\n",
    "                global_fold_classifier = BlendingMetamodel.from_disk(\n",
    "                    fold_id=-1,\n",
    "                    metamodel_name=model_name,\n",
    "                    gene_locus=gene_locus,\n",
    "                    target_obs_column=target_obs_column,\n",
    "                    base_model_train_fold_name=base_model_train_fold_name,\n",
    "                    metamodel_fold_label_train=metamodel_fold_label_train,\n",
    "                    metamodel_flavor=metamodel_flavor,\n",
    "                    # We are providing a metamodel_config directly to avoid expensive load from disk.\n",
    "                    # But we are cheating here, because our metamodel_config was created with use_stubs_instead_of_submodels=True\n",
    "                    # This means that the metamodel itself will be loaded from disk, but it won't be able to make new predictions because the submodels are nonexistent.\n",
    "                    # This is fine for our purposes here, and saves us a ton of time.\n",
    "                    metamodel_config=metamodel_config,\n",
    "                )\n",
    "            except FileNotFoundError as err:\n",
    "                # Global fold classifier does not exist,\n",
    "                # either because it was not trained yet,\n",
    "                # or because we may be using a single fold 0 (if config.cross_validation_split_strategy.value.is_single_fold_only is True).\n",
    "                logger.warning(\n",
    "                    f\"No global fold classifier found for {model_name}: {err}\"\n",
    "                )\n",
    "                global_fold_classifier = None\n",
    "\n",
    "            analyze_feature_importances(\n",
    "                model_name=model_name,\n",
    "                model_global_performance=model_global_performance,\n",
    "                gene_locus=gene_locus,\n",
    "                target_obs_column=target_obs_column,\n",
    "                metamodel_flavor=metamodel_flavor,\n",
    "                highres_results_output_prefix=highres_results_output_prefix,\n",
    "                global_fold_classifier=global_fold_classifier,\n",
    "            )\n",
    "\n",
    "            # Plot additional model diagnostics for models with internal cross validation over a range of hyperparameters\n",
    "            if any(\n",
    "                model_name.startswith(s)\n",
    "                for s in [\"lasso_cv\", \"ridge_cv\", \"elasticnet_cv\"]\n",
    "            ):\n",
    "                if \"_lambda1se\" in model_name:\n",
    "                    # TODO(bugfix): Remove this when we fix the bug in switch_lambda() that doesn't copy n_train_samples_ for plot_cross_validation_curve()\n",
    "                    # This workaround is ok for now, beacause the lambda1se model plot is going to be identical to the other model plot\n",
    "                    continue\n",
    "\n",
    "                def _get_classifiers():\n",
    "                    # load classifier from disk.\n",
    "                    for (\n",
    "                        fold_id,\n",
    "                        per_fold_performance,\n",
    "                    ) in model_global_performance.per_fold_outputs.items():\n",
    "                        yield (fold_id, per_fold_performance.classifier)\n",
    "                    if global_fold_classifier is not None:\n",
    "                        # Also include global fold classifier, if it exists\n",
    "                        yield (-1, global_fold_classifier)\n",
    "\n",
    "                for fold_id, clf in _get_classifiers():\n",
    "                    if isinstance(clf, BlendingMetamodel):\n",
    "                        # Unwrap if it's a BlendingMetamodel\n",
    "                        clf = clf._inner\n",
    "\n",
    "                    # it's probably a Pipeline - unwrap it\n",
    "                    clf = crosseval._get_final_estimator_if_pipeline(clf)\n",
    "\n",
    "                    if not isinstance(clf, GlmnetLogitNetWrapper):\n",
    "                        # it should be a GlmnetLogitNetWrapper - skip\n",
    "                        # this may be a glmnet model inside another wrapper, like an OvR wrapper.\n",
    "                        logger.info(\n",
    "                            f\"Expected {model_name} for fold {fold_id} to be of type GlmnetLogitNetWrapper, got {type(clf)}. Skipping plot_cross_validation_curve().\"\n",
    "                        )\n",
    "                        continue\n",
    "\n",
    "                    # TODO(bugfix): Scorer name should be stored in clf automatically. (Remove the hardcoded scorer_name here once we fix the scorer name bug.)\n",
    "                    # In internal/nested cross validation, we optimize MCC for metamodel, but AUC for base models. See discussion in core code\n",
    "                    fig = clf.plot_cross_validation_curve(scorer_name=\"MCC\")\n",
    "                    genetools.plots.savefig(\n",
    "                        fig,\n",
    "                        f\"{highres_results_output_prefix}.internal_cross_validation_hyperparameter_diagnostics.{model_name}.fold_{fold_id}.png\",\n",
    "                        dpi=300,\n",
    "                    )\n",
    "                    plt.close(fig)\n",
    "\n",
    "    except Exception as err:\n",
    "        logger.exception(\n",
    "            f\"{gene_locus}, {target_obs_column}, metamodel flavor {metamodel_flavor}, config {metamodel_config} failed with error: {err}\"\n",
    "        )\n",
    "\n",
    "    return computed_abstentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c533b66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis(gene_locus: GeneLocus, target_obs_column: TargetObsColumnEnum):\n",
    "    try:\n",
    "        flavors = train_metamodel.get_metamodel_flavors(\n",
    "            gene_locus=gene_locus,\n",
    "            target_obs_column=target_obs_column,\n",
    "            fold_id=config.all_fold_ids[0],\n",
    "            base_model_train_fold_name=base_model_train_fold_name,\n",
    "            # For the sake of speed, we are choosing not to load all the submodels from disk here.\n",
    "            use_stubs_instead_of_submodels=True,\n",
    "        )\n",
    "    except Exception as err:\n",
    "        logger.warning(\n",
    "            f\"Failed to generate metamodel flavors for {gene_locus}, {target_obs_column}: {err}\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    abstentions_from_default_flavor = None\n",
    "\n",
    "    # Process \"default\" first to get list of abstentions.\n",
    "    if \"default\" in flavors:\n",
    "        abstentions_from_default_flavor = analyze_metamodel_flavor(\n",
    "            gene_locus=gene_locus,\n",
    "            target_obs_column=target_obs_column,\n",
    "            metamodel_flavor=\"default\",\n",
    "            metamodel_config=flavors[\"default\"],\n",
    "            forced_abstentions=None,\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"abstentions_from_default_flavor = {abstentions_from_default_flavor}\"\n",
    "        )\n",
    "\n",
    "    # Process remaining flavors\n",
    "    for metamodel_flavor, metamodel_config in flavors.items():\n",
    "        if metamodel_flavor == \"default\":\n",
    "            # Already processed, skip\n",
    "            continue\n",
    "        analyze_metamodel_flavor(\n",
    "            gene_locus=gene_locus,\n",
    "            target_obs_column=target_obs_column,\n",
    "            metamodel_flavor=metamodel_flavor,\n",
    "            metamodel_config=metamodel_config,\n",
    "            forced_abstentions=abstentions_from_default_flavor,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8099f6-dbb2-4bd7-a16c-340717a1455e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c908199-eb65-48ec-bb28-c6ca7109406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual gene locus\n",
    "for gene_locus in config.gene_loci_used:\n",
    "    print(gene_locus)\n",
    "    GeneLocus.validate_single_value(gene_locus)\n",
    "    for target_obs_column in config.classification_targets:\n",
    "        run_analysis(gene_locus=gene_locus, target_obs_column=target_obs_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af424ab4-0646-44b5-8748-a595ca5f296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Together in combined metamodel\n",
    "if len(config.gene_loci_used) > 1:\n",
    "    print(config.gene_loci_used)\n",
    "    for target_obs_column in config.classification_targets:\n",
    "        run_analysis(\n",
    "            gene_locus=config.gene_loci_used, target_obs_column=target_obs_column\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17383aba-3778-4b20-ab43-69f29378d783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84024ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7f1653-339a-4c8f-9586-57fc8ddeda9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "py39-cuda-env",
   "language": "python",
   "name": "py39-cuda-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
