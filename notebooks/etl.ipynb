{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL\n",
    "\n",
    "**See dask worker logs on disk.**\n",
    "\n",
    "Convert csvs to parquet. The resulting parquet files are partitioned by `participant_label` and `specimen_label`, so we can run `df.map_partitions(lambda part: ...)` to execute a function on each specimen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T02:03:55.158536Z",
     "iopub.status.busy": "2023-01-14T02:03:55.157743Z",
     "iopub.status.idle": "2023-01-14T02:04:04.337311Z",
     "shell.execute_reply": "2023-01-14T02:04:04.335348Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import time\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from IPython.display import display\n",
    "from typing import Dict\n",
    "from malid import config\n",
    "from malid.datamodels import GeneLocus\n",
    "from malid.etl import (\n",
    "    dtypes_read_in,\n",
    "    dtypes_expected_after_preprocessing,\n",
    "    preprocess_each_participant_table,\n",
    "    fix_dtypes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T02:04:04.345127Z",
     "iopub.status.busy": "2023-01-14T02:04:04.344538Z",
     "iopub.status.idle": "2023-01-14T02:04:04.367488Z",
     "shell.execute_reply": "2023-01-14T02:04:04.365770Z"
    }
   },
   "outputs": [],
   "source": [
    "config.paths.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T02:04:04.374170Z",
     "iopub.status.busy": "2023-01-14T02:04:04.373626Z",
     "iopub.status.idle": "2023-01-14T02:04:15.079051Z",
     "shell.execute_reply": "2023-01-14T02:04:15.078183Z"
    }
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "dask.config.set({\"logging.distributed\": \"info\"})\n",
    "\n",
    "# multi-processing backend\n",
    "# access dashbaord at http://127.0.0.1:61083\n",
    "client = Client(\n",
    "    scheduler_port=61084,\n",
    "    dashboard_address=\":61083\",\n",
    "    n_workers=8,  # 4\n",
    "    processes=True,\n",
    "    threads_per_worker=8,\n",
    "    memory_limit=\"auto\",  # \"125GB\" per worker\n",
    "    local_directory=\"/tmp\",\n",
    ")\n",
    "\n",
    "\n",
    "def setup_worker_logging(dask_worker: dask.distributed.worker.Worker):\n",
    "    import malid\n",
    "    from notebooklog import setup_logger\n",
    "\n",
    "    malid.logger, log_fname = setup_logger(\n",
    "        log_dir=config.paths.log_dir, name=f\"dask_worker_{dask_worker.name}\"\n",
    "    )\n",
    "    malid.logger.info(log_fname)\n",
    "    print(log_fname)\n",
    "\n",
    "\n",
    "# Setup logging to disk on every current and future worker\n",
    "# https://stackoverflow.com/questions/41475239/how-to-set-up-logging-on-dask-distributed-workers\n",
    "client.register_worker_callbacks(setup=setup_worker_logging)\n",
    "\n",
    "display(client)\n",
    "# for debugging: client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T02:04:15.085892Z",
     "iopub.status.busy": "2023-01-14T02:04:15.085537Z",
     "iopub.status.idle": "2023-01-14T02:04:15.099487Z",
     "shell.execute_reply": "2023-01-14T02:04:15.098563Z"
    }
   },
   "outputs": [],
   "source": [
    "cols = {\n",
    "    GeneLocus.BCR: list(dtypes_read_in[GeneLocus.BCR].keys()),\n",
    "    GeneLocus.TCR: list(dtypes_read_in[GeneLocus.TCR].keys()),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to do `df = dd.read_csv(fnames, sep=\"\\t\", compression=\"bz2\", dtype=dtypes, usecols=cols)`, it works but with:\n",
    "\n",
    "```\n",
    "/home/maxim/miniconda3/lib/python3.7/site-packages/dask/dataframe/io/csv.py:459: UserWarning: Warning bz2 compression does not support breaking apart files\n",
    "Please ensure that each individual file can fit in memory and\n",
    "use the keyword ``blocksize=None to remove this message``\n",
    "Setting ``blocksize=None``\n",
    "  \"Setting ``blocksize=None``\" % compression\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T02:04:15.106988Z",
     "iopub.status.busy": "2023-01-14T02:04:15.106544Z",
     "iopub.status.idle": "2023-01-14T02:04:15.115966Z",
     "shell.execute_reply": "2023-01-14T02:04:15.114373Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = dd.read_csv(fnames, sep=\"\\t\", compression=\"bz2\", dtype=dtypes, usecols=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T02:04:15.122623Z",
     "iopub.status.busy": "2023-01-14T02:04:15.122069Z",
     "iopub.status.idle": "2023-01-14T02:04:15.132140Z",
     "shell.execute_reply": "2023-01-14T02:04:15.130300Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# manual load with special processing:\n",
    "# deduping and setting num_reads, setting extracted_isotype, setting disease and disease_subtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T02:04:15.139643Z",
     "iopub.status.busy": "2023-01-14T02:04:15.139055Z",
     "iopub.status.idle": "2023-01-14T02:04:15.148980Z",
     "shell.execute_reply": "2023-01-14T02:04:15.147145Z"
    }
   },
   "outputs": [],
   "source": [
    "allowed_hiv_runs = [\"M111\", \"M112\", \"M113\", \"M114\", \"M124\", \"M125\", \"M132\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T02:04:15.156657Z",
     "iopub.status.busy": "2023-01-14T02:04:15.156066Z",
     "iopub.status.idle": "2023-01-14T02:04:15.179272Z",
     "shell.execute_reply": "2023-01-14T02:04:15.177337Z"
    }
   },
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def load_participant(files: Dict[GeneLocus, str], metadata_whitelist: pd.DataFrame):\n",
    "    final_dtypes = dtypes_expected_after_preprocessing  # not dependent on locus\n",
    "    df_parts = []\n",
    "    for gene_locus, fname in files.items():\n",
    "        df_for_locus = pd.read_csv(\n",
    "            fname, sep=\"\\t\", dtype=dtypes_read_in[gene_locus], usecols=cols[gene_locus]\n",
    "        )\n",
    "\n",
    "        # filter out anything except whitelisted specimens\n",
    "        # this means df.shape[0] can become 0\n",
    "        df_for_locus = pd.merge(\n",
    "            df_for_locus,\n",
    "            metadata_whitelist,\n",
    "            how=\"inner\",\n",
    "            on=[\"participant_label\", \"specimen_label\"],\n",
    "        )\n",
    "\n",
    "        if df_for_locus.shape[0] == 0:\n",
    "            # empty sample at this point - skip rest of processing this locus\n",
    "            continue\n",
    "\n",
    "        # override some variables\n",
    "        df_for_locus[\"participant_label\"] = df_for_locus[\n",
    "            \"participant_label_override\"\n",
    "        ].fillna(df_for_locus[\"participant_label\"])\n",
    "        df_for_locus[\"specimen_time_point\"] = df_for_locus[\n",
    "            \"specimen_time_point_override\"\n",
    "        ].fillna(df_for_locus[\"specimen_time_point\"])\n",
    "\n",
    "        # if this is a patient from the HIV cohort: allow specimens from certain runs only\n",
    "        if (\n",
    "            df_for_locus.shape[0] > 0 and df_for_locus[\"hiv_run_filter\"].iloc[0] == True\n",
    "        ):  # must check shape[0] > 0 so iloc[0] does not fail\n",
    "            # select certain run IDs only. exclude very old runs (M52 and such)\n",
    "            # this means df.shape[0] can become 0\n",
    "            df_for_locus = df_for_locus.loc[\n",
    "                df_for_locus[\"run_label\"].isin(allowed_hiv_runs)\n",
    "            ]\n",
    "\n",
    "        df_parts.append(\n",
    "            preprocess_each_participant_table(\n",
    "                df=df_for_locus.reset_index(drop=True),\n",
    "                gene_locus=gene_locus,\n",
    "                final_dtypes=final_dtypes,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # combine BCR + TCR data from same participant. necessary because we output one parquet partition per specimen - including both loci\n",
    "    if len(df_parts) == 0:\n",
    "        # return empty dataframe but with the right columns + dtypes\n",
    "        return fix_dtypes(pd.DataFrame(), final_dtypes)\n",
    "\n",
    "    return pd.concat(df_parts, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T02:04:15.186608Z",
     "iopub.status.busy": "2023-01-14T02:04:15.186018Z",
     "iopub.status.idle": "2023-01-14T02:04:15.201997Z",
     "shell.execute_reply": "2023-01-14T02:04:15.199967Z"
    }
   },
   "outputs": [],
   "source": [
    "bcr_directories_to_read = [\n",
    "    f\"{config.paths.base_data_dir}/hhc_bcr_part_tables/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/hiv_bcr_part_tables/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/covid19_buffycoat/bcr/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/M418_M434_Covid_SamYang/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/M447_M448_pediatric_lupus/BCR_M447/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/M454_M455_adult_lupus_rna/BCR_M454/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/M456_M457_adult_lupus_paxgene/BCR_M456/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/M464_M463_healthy_children/BCR_M465/part_table_*.bz2\",\n",
    "    # These datasets are BCR only:\n",
    "    f\"{config.paths.base_data_dir}/covid19_seattle/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/lupus_m281redo/part_table_*.bz2\",\n",
    "]\n",
    "tcr_directories_to_read = [\n",
    "    f\"{config.paths.base_data_dir}/hhc_tcr_part_tables/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/hiv_tcr_part_tables/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/covid19_buffycoat/tcr/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/M419_Covid_SamYang_tcrb/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/M447_M448_pediatric_lupus/TCR_M448/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/M454_M455_adult_lupus_rna/TCR_M455/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/M456_M457_adult_lupus_paxgene/TCR_M457/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/M464_M463_healthy_children/TCR_M463/part_table_*.bz2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T02:04:15.211933Z",
     "iopub.status.busy": "2023-01-14T02:04:15.209345Z",
     "iopub.status.idle": "2023-01-14T02:04:15.573288Z",
     "shell.execute_reply": "2023-01-14T02:04:15.570658Z"
    }
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for gene_locus, locus_dirs in zip(\n",
    "    [GeneLocus.BCR, GeneLocus.TCR], [bcr_directories_to_read, tcr_directories_to_read]\n",
    "):\n",
    "    for dirname in locus_dirs:\n",
    "        fnames = list(glob.glob(dirname))\n",
    "        if len(fnames) == 0:\n",
    "            # The path must be wrong\n",
    "            raise ValueError(f\"No part tables found in {dirname} for {gene_locus}\")\n",
    "        dfs.append(pd.DataFrame({\"fname_full\": fnames, \"gene_locus\": gene_locus.name}))\n",
    "\n",
    "files = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "files[\"fname_trim\"] = files[\"fname_full\"].apply(os.path.basename)\n",
    "files.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T02:04:15.580434Z",
     "iopub.status.busy": "2023-01-14T02:04:15.579956Z",
     "iopub.status.idle": "2023-01-14T02:04:15.603206Z",
     "shell.execute_reply": "2023-01-14T02:04:15.602387Z"
    }
   },
   "outputs": [],
   "source": [
    "files.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T02:04:15.609996Z",
     "iopub.status.busy": "2023-01-14T02:04:15.609535Z",
     "iopub.status.idle": "2023-01-14T02:04:15.615438Z",
     "shell.execute_reply": "2023-01-14T02:04:15.614359Z"
    }
   },
   "outputs": [],
   "source": [
    "# # debug only:\n",
    "# # files = files.iloc[-10:]\n",
    "# files = files.sort_values(\"fname_trim\").iloc[:4]\n",
    "# files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T02:04:15.622252Z",
     "iopub.status.busy": "2023-01-14T02:04:15.621636Z",
     "iopub.status.idle": "2023-01-14T02:04:15.666935Z",
     "shell.execute_reply": "2023-01-14T02:04:15.666342Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: switch to helpers._load_etl_metadata()\n",
    "specimen_whitelist_and_metadata = pd.read_csv(\n",
    "    f\"{config.paths.metadata_dir}/generated_combined_specimen_metadata.tsv\",\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "specimen_whitelist_and_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T02:04:15.672462Z",
     "iopub.status.busy": "2023-01-14T02:04:15.672257Z",
     "iopub.status.idle": "2023-01-14T02:04:15.681658Z",
     "shell.execute_reply": "2023-01-14T02:04:15.681003Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter to matching participant labels, so we're not loading part tables only to throw them out completely\n",
    "# we might still throw them out partially (some specimens)\n",
    "assert not specimen_whitelist_and_metadata[\"participant_label\"].isna().any()\n",
    "specimen_whitelist_and_metadata[\"fname\"] = (\n",
    "    \"part_table_\" + specimen_whitelist_and_metadata[\"participant_label\"] + \".bz2\"\n",
    ")\n",
    "specimen_whitelist_and_metadata[\"fname\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T02:04:15.688252Z",
     "iopub.status.busy": "2023-01-14T02:04:15.687931Z",
     "iopub.status.idle": "2023-01-14T02:04:15.698590Z",
     "shell.execute_reply": "2023-01-14T02:04:15.697559Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "specimen_whitelist_and_metadata[\"fname\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T02:04:15.704487Z",
     "iopub.status.busy": "2023-01-14T02:04:15.703999Z",
     "iopub.status.idle": "2023-01-14T02:04:15.723391Z",
     "shell.execute_reply": "2023-01-14T02:04:15.721697Z"
    }
   },
   "outputs": [],
   "source": [
    "files_trimmed = pd.merge(\n",
    "    files,  # left side will have one row per locus per participant\n",
    "    specimen_whitelist_and_metadata,  # right side will have one row per specimen per participant\n",
    "    left_on=\"fname_trim\",\n",
    "    right_on=\"fname\",\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "assert (\n",
    "    files_trimmed[\"fname_trim\"].nunique()\n",
    "    == specimen_whitelist_and_metadata[\"fname\"].nunique()\n",
    "), \"Some expected part tables are missing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T02:04:15.730300Z",
     "iopub.status.busy": "2023-01-14T02:04:15.729905Z",
     "iopub.status.idle": "2023-01-14T02:04:15.740353Z",
     "shell.execute_reply": "2023-01-14T02:04:15.738515Z"
    }
   },
   "outputs": [],
   "source": [
    "files_trimmed[\"fname_trim\"].nunique(), files_trimmed.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T02:04:15.748107Z",
     "iopub.status.busy": "2023-01-14T02:04:15.747517Z",
     "iopub.status.idle": "2023-01-14T02:04:16.458156Z",
     "shell.execute_reply": "2023-01-14T02:04:16.456048Z"
    }
   },
   "outputs": [],
   "source": [
    "# all Delayed() objects\n",
    "part_tables = []\n",
    "\n",
    "for key, grp in files_trimmed.groupby(\"fname_trim\"):\n",
    "    # We have now selected all files for this participant\n",
    "    # Spread out over several rows by locus and by specimen - even though ultimately there is one source file on disk per locus per participant\n",
    "    # Drop specimen dupes:\n",
    "    unique_locus_files_for_this_participant = (\n",
    "        grp[[\"fname_full\", \"gene_locus\"]]\n",
    "        .drop_duplicates()\n",
    "        .set_index(\"gene_locus\")[\"fname_full\"]\n",
    "    )\n",
    "    if unique_locus_files_for_this_participant.index.duplicated().any():\n",
    "        raise ValueError(\n",
    "            \"Multiple unique files on disk for the same locus for the same participant - should be one file per locus per participant\"\n",
    "        )\n",
    "    part_tables.append(\n",
    "        load_participant(\n",
    "            files={\n",
    "                GeneLocus[locus_name]: fname\n",
    "                for locus_name, fname in unique_locus_files_for_this_participant.to_dict().items()\n",
    "            },\n",
    "            metadata_whitelist=specimen_whitelist_and_metadata,\n",
    "        )\n",
    "    )\n",
    "\n",
    "df = dd.from_delayed(\n",
    "    part_tables, meta=dtypes_expected_after_preprocessing, verify_meta=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T02:04:16.465222Z",
     "iopub.status.busy": "2023-01-14T02:04:16.464783Z",
     "iopub.status.idle": "2023-01-14T02:04:16.512906Z",
     "shell.execute_reply": "2023-01-14T02:04:16.512270Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T02:04:16.515570Z",
     "iopub.status.busy": "2023-01-14T02:04:16.515373Z",
     "iopub.status.idle": "2023-01-14T02:04:16.519025Z",
     "shell.execute_reply": "2023-01-14T02:04:16.518124Z"
    }
   },
   "outputs": [],
   "source": [
    "itime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T02:04:16.522769Z",
     "iopub.status.busy": "2023-01-14T02:04:16.522526Z",
     "iopub.status.idle": "2023-01-14T03:12:08.289936Z",
     "shell.execute_reply": "2023-01-14T03:12:08.281926Z"
    }
   },
   "outputs": [],
   "source": [
    "# This can behave weirdly with empty partitions: https://github.com/dask/dask/issues/8832 - requires being careful with engine, schema, and metadata\n",
    "\n",
    "# fastparquet engine seems buggy, perhaps due to empty parititons too:\n",
    "# OverflowError: value too large to convert to int\n",
    "# Exception ignored in: 'fastparquet.cencoding.write_thrift'\n",
    "# Traceback (most recent call last):\n",
    "#   File \"/users/maximz/anaconda3/envs/cuda-env-py39/lib/python3.9/site-packages/fastparquet/writer.py\", line 1488, in write_thrift\n",
    "#     return f.write(obj.to_bytes())\n",
    "\n",
    "df.to_parquet(\n",
    "    config.paths.sequences,\n",
    "    overwrite=True,\n",
    "    compression=\"snappy\",  # gzip\n",
    "    engine=\"pyarrow\",\n",
    "    # schema arg only accepted by pyarrow engine:\n",
    "    # Set schema to \"infer\" if we have any empty partitions and using pyarrow.\n",
    "    # schema=\"infer\" is no longer slow as of https://github.com/dask/dask/pull/9131\n",
    "    # schema=None breaks downstream readers.\n",
    "    schema=\"infer\",\n",
    "    # also, do empty partitions even make it to disk, or are they eliminated? they seem eliminated.\n",
    "    write_metadata_file=False,\n",
    "    partition_on=[\"participant_label\", \"specimen_label\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T03:12:08.297987Z",
     "iopub.status.busy": "2023-01-14T03:12:08.297680Z",
     "iopub.status.idle": "2023-01-14T03:12:08.303617Z",
     "shell.execute_reply": "2023-01-14T03:12:08.302452Z"
    }
   },
   "outputs": [],
   "source": [
    "etime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T03:12:08.309500Z",
     "iopub.status.busy": "2023-01-14T03:12:08.309227Z",
     "iopub.status.idle": "2023-01-14T03:12:08.325764Z",
     "shell.execute_reply": "2023-01-14T03:12:08.323976Z"
    }
   },
   "outputs": [],
   "source": [
    "etime - itime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T03:12:08.332737Z",
     "iopub.status.busy": "2023-01-14T03:12:08.332140Z",
     "iopub.status.idle": "2023-01-14T03:12:10.596423Z",
     "shell.execute_reply": "2023-01-14T03:12:10.594649Z"
    }
   },
   "outputs": [],
   "source": [
    "df2 = dd.read_parquet(config.paths.sequences, engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T03:12:10.602337Z",
     "iopub.status.busy": "2023-01-14T03:12:10.602130Z",
     "iopub.status.idle": "2023-01-14T03:12:10.645998Z",
     "shell.execute_reply": "2023-01-14T03:12:10.644171Z"
    }
   },
   "outputs": [],
   "source": [
    "# check dtypes\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T03:12:10.651353Z",
     "iopub.status.busy": "2023-01-14T03:12:10.651174Z",
     "iopub.status.idle": "2023-01-14T03:12:10.663273Z",
     "shell.execute_reply": "2023-01-14T03:12:10.661297Z"
    }
   },
   "outputs": [],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T03:12:10.669157Z",
     "iopub.status.busy": "2023-01-14T03:12:10.668834Z",
     "iopub.status.idle": "2023-01-14T03:12:10.681134Z",
     "shell.execute_reply": "2023-01-14T03:12:10.679038Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T03:12:10.688129Z",
     "iopub.status.busy": "2023-01-14T03:12:10.687544Z",
     "iopub.status.idle": "2023-01-14T03:12:10.704524Z",
     "shell.execute_reply": "2023-01-14T03:12:10.701319Z"
    }
   },
   "outputs": [],
   "source": [
    "# expected higher because now divided by participant_label and specimen_label\n",
    "df.npartitions, df2.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T03:12:10.713170Z",
     "iopub.status.busy": "2023-01-14T03:12:10.712623Z",
     "iopub.status.idle": "2023-01-14T03:12:10.721790Z",
     "shell.execute_reply": "2023-01-14T03:12:10.719893Z"
    }
   },
   "outputs": [],
   "source": [
    "# df2 = dd.read_parquet(config.paths.sequences, engine=\"fastparquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This warning `Partition names coerce to values of different types, e.g. ['M64-079', Timestamp('2039-01-01 00:00:54')]` is a serious problem for us; we need to avoid `fastparquet` as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T03:12:10.729074Z",
     "iopub.status.busy": "2023-01-14T03:12:10.728520Z",
     "iopub.status.idle": "2023-01-14T03:12:10.737492Z",
     "shell.execute_reply": "2023-01-14T03:12:10.735342Z"
    }
   },
   "outputs": [],
   "source": [
    "# # check dtypes\n",
    "# df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T03:12:10.750043Z",
     "iopub.status.busy": "2023-01-14T03:12:10.745426Z",
     "iopub.status.idle": "2023-01-14T03:12:10.757513Z",
     "shell.execute_reply": "2023-01-14T03:12:10.755110Z"
    }
   },
   "outputs": [],
   "source": [
    "# df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T03:12:10.765070Z",
     "iopub.status.busy": "2023-01-14T03:12:10.764496Z",
     "iopub.status.idle": "2023-01-14T03:12:10.776790Z",
     "shell.execute_reply": "2023-01-14T03:12:10.773628Z"
    }
   },
   "outputs": [],
   "source": [
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T03:12:10.788001Z",
     "iopub.status.busy": "2023-01-14T03:12:10.787405Z",
     "iopub.status.idle": "2023-01-14T03:12:15.188649Z",
     "shell.execute_reply": "2023-01-14T03:12:15.186564Z"
    }
   },
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "py39-cuda-env",
   "language": "python",
   "name": "py39-cuda-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
