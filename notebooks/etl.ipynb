{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL\n",
    "\n",
    "**See dask worker logs on disk.**\n",
    "\n",
    "Convert csvs to parquet. The resulting parquet files are partitioned by `participant_label` and `specimen_label`, so we can run `df.map_partitions(lambda part: ...)` to execute a function on each specimen.\n",
    "\n",
    "The parquet dataset will include all datasets: in-house and Adaptive together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import time\n",
    "import dask, dask.distributed\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "from IPython.display import display\n",
    "from typing import Dict\n",
    "from malid import config\n",
    "from malid.datamodels import GeneLocus\n",
    "from malid.etl import (\n",
    "    dtypes_expected_after_preprocessing,\n",
    "    preprocess_each_participant_table,\n",
    "    load_participant_data_external,\n",
    "    read_boydlab_participant_table,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.paths.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-processing backend\n",
    "# if already opened from another notebook, see https://stackoverflow.com/questions/60115736/dask-how-to-connect-to-running-cluster-scheduler-and-access-total-occupancy\n",
    "client = Client(\n",
    "    scheduler_port=config.dask_scheduler_port,\n",
    "    dashboard_address=config.dask_dashboard_address,\n",
    "    n_workers=config.dask_n_workers,\n",
    "    processes=True,\n",
    "    threads_per_worker=8,\n",
    "    # memory_limit=\"auto\",\n",
    "    # Still experimenting with this:\n",
    "    memory_limit=0,  # no limit\n",
    "    local_directory=\"/tmp\",\n",
    ")\n",
    "\n",
    "\n",
    "def setup_worker_logging(dask_worker: dask.distributed.worker.Worker):\n",
    "    import malid\n",
    "    from notebooklog import setup_logger\n",
    "\n",
    "    malid.logger, log_fname = setup_logger(\n",
    "        log_dir=config.paths.log_dir, name=f\"dask_worker_{dask_worker.name}\"\n",
    "    )\n",
    "    malid.logger.info(log_fname)\n",
    "    print(log_fname)\n",
    "\n",
    "\n",
    "# Setup logging to disk on every current and future worker\n",
    "# https://stackoverflow.com/questions/41475239/how-to-set-up-logging-on-dask-distributed-workers\n",
    "client.register_worker_callbacks(setup=setup_worker_logging)\n",
    "\n",
    "display(client)\n",
    "# for debugging: client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to do `df = dd.read_csv(fnames, sep=\"\\t\", compression=\"bz2\", dtype=dtypes, usecols=cols)`, it works but with:\n",
    "\n",
    "```\n",
    "/home/maxim/miniconda3/lib/python3.7/site-packages/dask/dataframe/io/csv.py:459: UserWarning: Warning bz2 compression does not support breaking apart files\n",
    "Please ensure that each individual file can fit in memory and\n",
    "use the keyword ``blocksize=None to remove this message``\n",
    "Setting ``blocksize=None``\n",
    "  \"Setting ``blocksize=None``\" % compression\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = dd.read_csv(fnames, sep=\"\\t\", compression=\"bz2\", dtype=dtypes, usecols=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# manual load with special processing:\n",
    "# deduping and setting num_reads, setting extracted_isotype, setting disease and disease_subtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def load_participant(files: Dict[GeneLocus, str], metadata_whitelist: pd.DataFrame):\n",
    "    df_parts = []\n",
    "    for gene_locus, fname in files.items():\n",
    "        df_parts.append(\n",
    "            preprocess_each_participant_table(\n",
    "                df=read_boydlab_participant_table(fname, gene_locus),\n",
    "                gene_locus=gene_locus,\n",
    "                metadata_whitelist=metadata_whitelist,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # combine BCR + TCR data from same participant.\n",
    "    # necessary because we output one parquet partition per specimen - including both loci.\n",
    "    # note that any or all parts may be empty dataframes (with .shape[0] == 0), but that's ok, as long as the columns and dtypes are correct.\n",
    "    return pd.concat(df_parts, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcr_directories_to_read = [\n",
    "    # NOTE: Some of the HHCs have been renamed as \".bz2.bak\" in hhc_bcr_part_tables,\n",
    "    # because they were resequenced later and had part tables reexported in other run directories (e.g. M477/M482).\n",
    "    f\"{config.paths.base_data_dir}/hhc_bcr_part_tables/part_table_*.bz2\",\n",
    "    #\n",
    "    f\"{config.paths.base_data_dir}/hiv_bcr_part_tables/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/covid19_buffycoat/bcr/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/M418_M434_Covid_SamYang/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/M447_M448_pediatric_lupus/BCR_M447/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/M454_M455_adult_lupus_rna/BCR_M454/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/M456_M457_adult_lupus_paxgene/BCR_M456/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/M464_M463_healthy_children/BCR_M465/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/M477_M482_yoni_ibd_and_some_old_hhc/BCR_M477/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/M479_M484_gubatan_ibd_and_some_old_hhc/BCR_M479/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/M433_M435_UPENN_Influenza_Study_2021/BCR_M433_M435/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/M491_M493_diabetes_biobank/BCR_M491_M492/part_table_*.bz2\",\n",
    "    # These datasets are BCR only:\n",
    "    f\"{config.paths.base_data_dir}/covid19_seattle/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/lupus_m281redo/part_table_*.bz2\",\n",
    "]\n",
    "tcr_directories_to_read = [\n",
    "    # NOTE: Some of the HHCs have been renamed as \".bz2.bak\" in hhc_tcr_part_tables,\n",
    "    # because they were resequenced later and had part tables reexported in other run directories.\n",
    "    f\"{config.paths.base_data_dir}/hhc_tcr_part_tables/part_table_*.bz2\",\n",
    "    #\n",
    "    f\"{config.paths.base_data_dir}/hiv_tcr_part_tables/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/covid19_buffycoat/tcr/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/M419_Covid_SamYang_tcrb/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/M447_M448_pediatric_lupus/TCR_M448/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/M454_M455_adult_lupus_rna/TCR_M455/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/M456_M457_adult_lupus_paxgene/TCR_M457/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/M464_M463_healthy_children/TCR_M463/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/M477_M482_yoni_ibd_and_some_old_hhc/TCR_M482/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/M479_M484_gubatan_ibd_and_some_old_hhc/TCR_M484/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/M433_M435_UPENN_Influenza_Study_2021/TCR_M444/part_table_*.bz2\",\n",
    "    f\"{config.paths.base_data_dir}/M491_M493_diabetes_biobank/TCR_M493/part_table_*.bz2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for gene_locus, locus_dirs in zip(\n",
    "    [GeneLocus.BCR, GeneLocus.TCR], [bcr_directories_to_read, tcr_directories_to_read]\n",
    "):\n",
    "    for dirname in locus_dirs:\n",
    "        fnames = list(glob.glob(dirname))\n",
    "        if len(fnames) == 0:\n",
    "            # The path must be wrong\n",
    "            raise ValueError(f\"No part tables found in {dirname} for {gene_locus}\")\n",
    "        dfs.append(pd.DataFrame({\"fname_full\": fnames, \"gene_locus\": gene_locus.name}))\n",
    "\n",
    "files = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "files[\"fname_trim\"] = files[\"fname_full\"].apply(os.path.basename)\n",
    "files.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # debug only:\n",
    "# # files = files.iloc[-10:]\n",
    "# files = files.sort_values(\"fname_trim\").iloc[:4]\n",
    "# files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: switch to helpers._load_etl_metadata()\n",
    "specimen_whitelist_and_metadata = pd.read_csv(\n",
    "    f\"{config.paths.metadata_dir}/generated_combined_specimen_metadata.tsv\",\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "specimen_whitelist_and_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to matching participant labels, so we're not loading part tables only to throw them out completely\n",
    "# we might still throw them out partially (some specimens)\n",
    "assert not specimen_whitelist_and_metadata[\"participant_label\"].isna().any()\n",
    "specimen_whitelist_and_metadata[\"fname\"] = (\n",
    "    \"part_table_\" + specimen_whitelist_and_metadata[\"participant_label\"] + \".bz2\"\n",
    ")\n",
    "specimen_whitelist_and_metadata[\"fname\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "specimen_whitelist_and_metadata[\"fname\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_trimmed = pd.merge(\n",
    "    files,  # left side will have one row per locus per participant\n",
    "    specimen_whitelist_and_metadata,  # right side will have one row per specimen per participant\n",
    "    left_on=\"fname_trim\",\n",
    "    right_on=\"fname\",\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "assert (\n",
    "    files_trimmed[\"fname_trim\"].nunique()\n",
    "    == specimen_whitelist_and_metadata[\"fname\"].nunique()\n",
    "), \"Some expected part tables are missing: \" + str(\n",
    "    set(specimen_whitelist_and_metadata[\"fname\"]) - set(files_trimmed[\"fname_trim\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_trimmed[\"fname_trim\"].nunique(), files_trimmed.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Adaptive metadata\n",
    "adaptive_metadata = pd.read_csv(\n",
    "    config.paths.metadata_dir / \"adaptive\" / \"generated.adaptive_external_cohorts.tsv\",\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "adaptive_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e69e93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load other external cohort metadata\n",
    "other_external_metadata = pd.read_csv(\n",
    "    config.paths.metadata_dir / \"generated.external_cohorts.tsv\",\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "other_external_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all Delayed() objects\n",
    "part_tables = []\n",
    "\n",
    "# in-house data\n",
    "for key, grp in files_trimmed.groupby(\"fname_trim\"):\n",
    "    # We have now selected all files for this participant, because fname_trim is something like part_table_BFI-#######.bz2 (there's a BCR file with that name and a TCR file with that name).\n",
    "    # The participant is spread out over several rows in files_trimmed by locus and by specimen - even though ultimately there is one source file on disk per locus per participant.\n",
    "\n",
    "    # Drop specimen dupes:\n",
    "    unique_locus_files_for_this_participant = (\n",
    "        grp[[\"fname_full\", \"gene_locus\"]]\n",
    "        .drop_duplicates()\n",
    "        .set_index(\"gene_locus\")[\"fname_full\"]\n",
    "    )\n",
    "    if unique_locus_files_for_this_participant.index.duplicated().any():\n",
    "        raise ValueError(\n",
    "            \"Multiple unique files on disk for the same locus for the same participant - should be one file per locus per participant\"\n",
    "        )\n",
    "    part_tables.append(\n",
    "        load_participant(\n",
    "            files={\n",
    "                GeneLocus[locus_name]: fname\n",
    "                for locus_name, fname in unique_locus_files_for_this_participant.to_dict().items()\n",
    "            },\n",
    "            metadata_whitelist=specimen_whitelist_and_metadata,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Adaptive data (TCR)\n",
    "delayed_load_func = dask.delayed(load_participant_data_external)\n",
    "for key, grp in adaptive_metadata.groupby(\"participant_label\"):\n",
    "    part_tables.append(\n",
    "        delayed_load_func(\n",
    "            participant_samples=grp,\n",
    "            gene_locus=GeneLocus.TCR,\n",
    "            base_path=config.paths.external_raw_data / \"adaptive_immuneaccess\",\n",
    "            is_adaptive=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Other external data (BCR or TCR)\n",
    "for key, grp in other_external_metadata.groupby(\"participant_label\"):\n",
    "    gene_locus = grp[\"gene_locus\"].unique()\n",
    "    # Allow some studies to be exempted from read count column requirements\n",
    "    expect_a_read_count_column = grp[\"expect_a_read_count_column\"].unique()\n",
    "    # Allow custom file extensions for some studies. Default is tsv\n",
    "    file_extension = grp[\"file_extension\"].unique()\n",
    "\n",
    "    # All participants are either BCR or TCR, not both\n",
    "    assert len(gene_locus) == 1\n",
    "    gene_locus = gene_locus[0]\n",
    "\n",
    "    # Other columns should also have single value\n",
    "    assert len(expect_a_read_count_column) == 1\n",
    "    expect_a_read_count_column = expect_a_read_count_column[0]\n",
    "    assert len(file_extension) == 1\n",
    "    file_extension = file_extension[0]\n",
    "\n",
    "    part_tables.append(\n",
    "        delayed_load_func(\n",
    "            participant_samples=grp,\n",
    "            # convert back from name attribute to full GeneLocus object\n",
    "            gene_locus=GeneLocus[gene_locus],\n",
    "            # Under this main directory are /study_name folders that include the samples and the parsed.IgH.tsv or parsed.TCRB.tsv files\n",
    "            base_path=config.paths.external_raw_data,\n",
    "            is_adaptive=False,\n",
    "            expect_a_read_count_column=expect_a_read_count_column,\n",
    "            file_extension=file_extension,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Later, consider giving the dask.delayed objects custom names, e.g. the participant label as name, so we can identify them in dashboard and track down errors.\n",
    "# see https://docs.dask.org/en/latest/delayed-api.html#dask.delayed.delayed and https://docs.dask.org/en/stable/custom-collections.html#implementing-deterministic-hashing\n",
    "# Perhaps this will also enable us to rearrange the order in which jobs are run. We had tried to randomly shuffle the part_tables list, but from_delayed seemed to ignore our shuffling. Are the jobs run in the order of their (currently random) dask.delayed name attributes?\n",
    "#\n",
    "# Easiest way to do this may be:\n",
    "# delayed_task = delayed_load_func(...)\n",
    "# delayed_task.key = f\"{delayed_task.key}_{gene_locus}_{grp['participant_label'].iloc[0]}\"\n",
    "# part_tables.append(delayed_task)\n",
    "# This appens specific information to the task's existing key, which Dask has already made sure is unique. Therefore we get unique identifier plus our own custom information.\n",
    "#\n",
    "# Alternative:\n",
    "# delayed_task = delayed(myfunc_not_yet_wrapped, dask_key_name=f\"load_data_{gene_locus}_{grp['participant_label'].iloc[0]}\")(...)\n",
    "# part_tables.append(delayed_task)\n",
    "# Here we directly control the task's key, but it's on us to make sure keys are unique and never reused.\n",
    "#\n",
    "# (We've tested neither approach ourselves.)\n",
    "\n",
    "df = dd.from_delayed(\n",
    "    part_tables, meta=dtypes_expected_after_preprocessing, verify_meta=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.paths.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can behave weirdly with empty partitions: https://github.com/dask/dask/issues/8832 - requires being careful with engine, schema, and metadata\n",
    "\n",
    "# fastparquet engine seems buggy, perhaps due to empty parititons too:\n",
    "# OverflowError: value too large to convert to int\n",
    "# Exception ignored in: 'fastparquet.cencoding.write_thrift'\n",
    "# Traceback (most recent call last):\n",
    "#   File \"/users/maximz/anaconda3/envs/cuda-env-py39/lib/python3.9/site-packages/fastparquet/writer.py\", line 1488, in write_thrift\n",
    "#     return f.write(obj.to_bytes())\n",
    "\n",
    "df.to_parquet(\n",
    "    config.paths.sequences,\n",
    "    overwrite=True,\n",
    "    compression=\"snappy\",  # gzip\n",
    "    engine=\"pyarrow\",\n",
    "    # schema arg only accepted by pyarrow engine:\n",
    "    # Set schema to \"infer\" if we have any empty partitions and using pyarrow.\n",
    "    # schema=\"infer\" is no longer slow as of https://github.com/dask/dask/pull/9131\n",
    "    # schema=None breaks downstream readers.\n",
    "    schema=\"infer\",\n",
    "    # also, do empty partitions even make it to disk, or are they eliminated? they seem eliminated.\n",
    "    write_metadata_file=False,\n",
    "    partition_on=[\"participant_label\", \"specimen_label\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etime - itime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = dd.read_parquet(config.paths.sequences, engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dtypes\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare dtypes\n",
    "pd.concat(\n",
    "    [\n",
    "        df.dtypes.rename(\"expected dtypes\"),\n",
    "        df2.dtypes.rename(\"reloaded observed dtypes\"),\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected higher because now divided by participant_label and specimen_label\n",
    "df.npartitions, df2.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = dd.read_parquet(config.paths.sequences, engine=\"fastparquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This warning `Partition names coerce to values of different types, e.g. ['M64-079', Timestamp('2039-01-01 00:00:54')]` is a serious problem for us; we need to avoid `fastparquet` as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check dtypes\n",
    "# df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "py39-cuda-env",
   "language": "python",
   "name": "py39-cuda-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
